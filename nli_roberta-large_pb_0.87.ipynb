{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1644155659595,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "y5joRz5dWOJJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3512,
     "status": "ok",
     "timestamp": 1644155666125,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "saedTdX9XVyl"
   },
   "outputs": [],
   "source": [
    "# ------ LIBRARY -------#\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "# torch\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
    "#\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch_optimizer as optim\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "#import time\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# transformer\n",
    "from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n",
    "from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n",
    "XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1644155666126,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "tu3duc0IbgYR"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\")\n",
    "test = pd.read_csv(\"data/test_data.csv\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1644155666127,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "IOmwy-ZUb8K3",
    "outputId": "8217e17b-fe08-4f6b-f9e7-d3bcdb39160d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>24993</td>\n",
       "      <td>오페라에 비하여 오라토리오에서는 독창보다도 합창이 중시되며, 테스토 또는 이스토리쿠...</td>\n",
       "      <td>오라토리오에서 테스토의 역할이 가장 중요하다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>24994</td>\n",
       "      <td>지하철역까지 걸어서 5분 정도 걸립니다.</td>\n",
       "      <td>지하철역까지 도보로 5분 정도 걸립니다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>24995</td>\n",
       "      <td>한편 이날 중앙방역대책본부는 집단 감염이 발생한 음식점 관련 역학조사 결과를 공개했다.</td>\n",
       "      <td>중악방역대책본부는 집단 감염과 관련한 모든 정보를 비공개했다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>24996</td>\n",
       "      <td>마미손이 랩을 하자 시청자들은 그의 정체를 파악했다.</td>\n",
       "      <td>시청자들은 마미손의 정체를 안다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>24997</td>\n",
       "      <td>집근처에 지하철역,버스정류장이 있기때문에 다른 곳으로 이동하는데 좋았습니다.</td>\n",
       "      <td>집 근처에 있는 지하철역을 이용하기 위해서는 걸어서 5분 정도 가야합니다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            premise  \\\n",
       "0          0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1          1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2          2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3          3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4          4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "...      ...                                                ...   \n",
       "24993  24993  오페라에 비하여 오라토리오에서는 독창보다도 합창이 중시되며, 테스토 또는 이스토리쿠...   \n",
       "24994  24994                             지하철역까지 걸어서 5분 정도 걸립니다.   \n",
       "24995  24995   한편 이날 중앙방역대책본부는 집단 감염이 발생한 음식점 관련 역학조사 결과를 공개했다.   \n",
       "24996  24996                      마미손이 랩을 하자 시청자들은 그의 정체를 파악했다.   \n",
       "24997  24997         집근처에 지하철역,버스정류장이 있기때문에 다른 곳으로 이동하는데 좋았습니다.   \n",
       "\n",
       "                                      hypothesis          label  \n",
       "0                                 씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                               자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2        예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                              원주민들은 종합대책에 만족했다.        neutral  \n",
       "4             이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  \n",
       "...                                          ...            ...  \n",
       "24993                  오라토리오에서 테스토의 역할이 가장 중요하다.        neutral  \n",
       "24994                     지하철역까지 도보로 5분 정도 걸립니다.     entailment  \n",
       "24995         중악방역대책본부는 집단 감염과 관련한 모든 정보를 비공개했다.  contradiction  \n",
       "24996                         시청자들은 마미손의 정체를 안다.     entailment  \n",
       "24997  집 근처에 있는 지하철역을 이용하기 위해서는 걸어서 5분 정도 가야합니다.        neutral  \n",
       "\n",
       "[24998 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1644155666515,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "HLpXmlVyb9K2",
    "outputId": "25b7235a-adce-4ee7-c368-8133949074f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# class args\n",
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    amp = True\n",
    "    gpu = '1'\n",
    "    \n",
    "    epochs=10\n",
    "    batch_size=16\n",
    "    weight_decay=1e-6\n",
    "    n_fold=5\n",
    "    fold=5 # [0, 1, 2, 3, 4] # 원래는 3\n",
    "    \n",
    "    exp_name = 'experiment_name_folder'\n",
    "    dir_ = f'./saved_models/'\n",
    "    pt = 'klue/roberta-large'\n",
    "    max_len = 100\n",
    "    \n",
    "    start_lr = 1e-5#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=2021\n",
    "    scheduler = None#'get_linear_schedule_with_warmup'\n",
    "\n",
    "\n",
    "data_dir = './'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
    "\n",
    "set_seeds(seed=args.seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1644155666516,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "aF8L7RYKuOWR",
    "outputId": "ab47b0aa-4f34-4d0a-caf3-1b3ba3d66a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction' 'entailment' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(train[\"label\"]))\n",
    "\n",
    "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9285,
     "status": "ok",
     "timestamp": 1644155676074,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "cEoMqloPuurY",
    "outputId": "805bfcf7-1eb3-4ede-9489-53485dcbfcf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(train.label):\n",
    "    train.label[i] = label_dict[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1644155676075,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "8hBhBeAdvQwW",
    "outputId": "e8150837-b543-43de-ba1f-52ec6ab5d669"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>24993</td>\n",
       "      <td>오페라에 비하여 오라토리오에서는 독창보다도 합창이 중시되며, 테스토 또는 이스토리쿠...</td>\n",
       "      <td>오라토리오에서 테스토의 역할이 가장 중요하다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>24994</td>\n",
       "      <td>지하철역까지 걸어서 5분 정도 걸립니다.</td>\n",
       "      <td>지하철역까지 도보로 5분 정도 걸립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>24995</td>\n",
       "      <td>한편 이날 중앙방역대책본부는 집단 감염이 발생한 음식점 관련 역학조사 결과를 공개했다.</td>\n",
       "      <td>중악방역대책본부는 집단 감염과 관련한 모든 정보를 비공개했다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>24996</td>\n",
       "      <td>마미손이 랩을 하자 시청자들은 그의 정체를 파악했다.</td>\n",
       "      <td>시청자들은 마미손의 정체를 안다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>24997</td>\n",
       "      <td>집근처에 지하철역,버스정류장이 있기때문에 다른 곳으로 이동하는데 좋았습니다.</td>\n",
       "      <td>집 근처에 있는 지하철역을 이용하기 위해서는 걸어서 5분 정도 가야합니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            premise  \\\n",
       "0          0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1          1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2          2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3          3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4          4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "...      ...                                                ...   \n",
       "24993  24993  오페라에 비하여 오라토리오에서는 독창보다도 합창이 중시되며, 테스토 또는 이스토리쿠...   \n",
       "24994  24994                             지하철역까지 걸어서 5분 정도 걸립니다.   \n",
       "24995  24995   한편 이날 중앙방역대책본부는 집단 감염이 발생한 음식점 관련 역학조사 결과를 공개했다.   \n",
       "24996  24996                      마미손이 랩을 하자 시청자들은 그의 정체를 파악했다.   \n",
       "24997  24997         집근처에 지하철역,버스정류장이 있기때문에 다른 곳으로 이동하는데 좋았습니다.   \n",
       "\n",
       "                                      hypothesis label  \n",
       "0                                 씨름의 여자들의 놀이이다.     1  \n",
       "1                               자작극을 벌인 이는 3명이다.     1  \n",
       "2        예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     0  \n",
       "3                              원주민들은 종합대책에 만족했다.     2  \n",
       "4             이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.     2  \n",
       "...                                          ...   ...  \n",
       "24993                  오라토리오에서 테스토의 역할이 가장 중요하다.     2  \n",
       "24994                     지하철역까지 도보로 5분 정도 걸립니다.     0  \n",
       "24995         중악방역대책본부는 집단 감염과 관련한 모든 정보를 비공개했다.     1  \n",
       "24996                         시청자들은 마미손의 정체를 안다.     0  \n",
       "24997  집 근처에 있는 지하철역을 이용하기 위해서는 걸어서 5분 정도 가야합니다.     2  \n",
       "\n",
       "[24998 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1644155676076,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "1SSkHU2_oWo0"
   },
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    # line = re.sub(r\"[^가-힣a-zA-Z0-9\\u4e00-\\u9fa5\\s]+\", \" \", line)\n",
    "    line = re.sub(r\"[^가-힣a-zA-Z0-9\\s]+\", \" \", line)\n",
    "    line = \" \".join(filter(lambda word: not word.isdigit(), line.split()))\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line).strip()\n",
    "    # line = line.lower()\n",
    "\n",
    "    if len(line) >= 2 and line[1] == \" \":\n",
    "        line = line[2:].strip()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 14441, 2073, 12382, 13169, 2200, 3797, 21505, 9005, 2259, 3997, 2031, 2079, 3661, 31221, 5845, 2200, 2112, 16, 5950, 15351, 17788, 7285, 748, 2088, 22048, 2470, 1132, 21893, 15351, 6481, 27135, 5417, 4084, 1972, 2145, 17524, 2138, 15526, 2259, 575, 28674, 18, 2, 14441, 2079, 3883, 2031, 2079, 5845, 28674, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.pt, use_fast=True)\n",
    "tokenizer(train['premise'][0], train['hypothesis'][0], max_length=args.max_len, \n",
    "                                                pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1644155676076,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "y18oNP_jeegx"
   },
   "outputs": [],
   "source": [
    "def preprocessing_train(data):\n",
    "    \n",
    "    pt = args.pt#'monologg/kobert'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pt, use_fast=True)\n",
    "    \n",
    "    MAX_LEN = args.max_len\n",
    "    train=data[['premise', 'hypothesis', 'label']]\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(train.iterrows()):\n",
    "#         line = preprocessing(row.premise)\n",
    "#         lines.append(line)\n",
    "#     train['premise_pre'] = lines\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(train.iterrows()):\n",
    "#         line = preprocessing(row.hypothesis)\n",
    "#         lines.append(line)\n",
    "#     train['hypothesis_pre'] = lines\n",
    "\n",
    "    input_ids =[]\n",
    "    attention_masks =[]\n",
    "    token_type_ids =[]\n",
    "    train_data_labels = []\n",
    "\n",
    "    for train_premise, train_hypothesis, train_label in tqdm.tqdm(zip(train['premise'], train['hypothesis'], train['label'])):\n",
    "        try:\n",
    "            train_sent = train_premise + \" \" + tokenizer.sep_token + \" \" + train_hypothesis\n",
    "            token= tokenizer(train_sent, max_length=args.max_len, \n",
    "                                                pad_to_max_length=True)\n",
    "\n",
    "            input_ids.append(token['input_ids'])\n",
    "            attention_masks.append(token['attention_mask'])\n",
    "            token_type_ids.append(token['token_type_ids'])\n",
    "            #########################################\n",
    "            train_data_labels.append(train_label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    train_input_ids=np.array(input_ids, dtype=int)\n",
    "    train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "    train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "    ###########################################################\n",
    "    train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
    "    train_labels=np.asarray(train_data_labels, dtype=np.int32)\n",
    "\n",
    "    # save\n",
    "    train_data = {}\n",
    "\n",
    "    train_data['input_ids'] = train_input_ids\n",
    "    train_data['attention_mask'] = train_attention_masks\n",
    "    train_data['token_type_ids'] = train_token_type_ids\n",
    "    train_data['targets'] = np.asarray(train_data_labels, dtype=np.int32)\n",
    "    \n",
    "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
    "    with open(f'./data/{pt}/train_data_{MAX_LEN}_sep.pickle', 'wb') as f:\n",
    "        pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1644155676077,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "qW1f2977ruEZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "24998it [00:05, 4386.45it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1644155676077,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "C_s9yG3lr6ha"
   },
   "outputs": [],
   "source": [
    "with open(f'./data/{args.pt}/train_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1644155676078,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "AcMKVufFsdxa",
    "outputId": "f560a737-9f57-47b4-ebc6-2eb1bbc659d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['token_type_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1644155676594,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "q77laOsvseoo"
   },
   "outputs": [],
   "source": [
    "def preprocessing_test(data):\n",
    "    \n",
    "    pt = args.pt#'monologg/kobert'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pt)\n",
    "    \n",
    "    MAX_LEN = args.max_len\n",
    "    test=data[['premise', 'hypothesis']]\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(test.iterrows()):\n",
    "#         line = preprocessing(row.premise)\n",
    "#         lines.append(line)\n",
    "#     test['premise_pre'] = lines\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(test.iterrows()):\n",
    "#         line = preprocessing(row.hypothesis)\n",
    "#         lines.append(line)\n",
    "#     test['hypothesis_pre'] = lines\n",
    "    \n",
    "    input_ids =[]\n",
    "    attention_masks =[]\n",
    "    token_type_ids =[]\n",
    "\n",
    "    for test_premise, test_hypothesis in tqdm.tqdm(zip(test['premise'], test['hypothesis'])):\n",
    "        try:\n",
    "            test_sent = test_premise + \" \" + tokenizer.sep_token + \" \" + test_hypothesis\n",
    "            token= tokenizer(test_sent, max_length=args.max_len, \n",
    "                                                pad_to_max_length=True)\n",
    "\n",
    "            input_ids.append(token['input_ids'])\n",
    "            attention_masks.append(token['attention_mask'])\n",
    "            token_type_ids.append(token['token_type_ids'])\n",
    "            #########################################\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "\n",
    "    test_input_ids=np.array(input_ids, dtype=int)\n",
    "    test_attention_masks=np.array(attention_masks, dtype=int)\n",
    "    test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "    ###########################################################\n",
    "    test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)\n",
    "\n",
    "\n",
    "    # save\n",
    "    test_data = {}\n",
    "\n",
    "    test_data['input_ids'] = test_input_ids\n",
    "    test_data['attention_mask'] = test_attention_masks\n",
    "    test_data['token_type_ids'] = test_token_type_ids\n",
    "    \n",
    "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
    "    with open(f'./data/{pt}/test_data_{MAX_LEN}_sep.pickle', 'wb') as f:\n",
    "        pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1644155676595,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "miraIC6Fw7qh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1666it [00:00, 4511.63it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1644155676597,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "EAkfqSa3w8oS"
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  dataset\n",
    "# ------------------------\n",
    "class NliDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, data, test=False):\n",
    "        \n",
    "        self.data = data\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        ids = torch.tensor(self.data['input_ids'][idx], dtype=torch.long)\n",
    "        mask = torch.tensor(self.data['attention_mask'][idx], dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(self.data['token_type_ids'][idx], dtype=torch.long)\n",
    "         \n",
    "            \n",
    "        if self.test:\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "                'token_type_ids': token_type_ids\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            target = torch.tensor(self.data['targets'][idx],dtype=torch.long)\n",
    "\n",
    "            return {\n",
    "                    'ids': ids,\n",
    "                    'mask': mask,\n",
    "                    'token_type_ids': token_type_ids,\n",
    "                    'targets': target\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1644155737762,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "z1u2xbHrSuo1"
   },
   "outputs": [],
   "source": [
    "# - util - #\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "def load_data():\n",
    "    train=pd.read_csv('data/train_data.csv')\n",
    "    test=pd.read_csv('data/test_data.csv')\n",
    "    \n",
    "    #\n",
    "    train=train[['premise', 'hypothesis', 'label']]\n",
    "    test=test[['premise', 'hypothesis']]\n",
    "    #\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    train['fold'] = -1\n",
    "    for n_fold, (_,v_idx) in enumerate(skf.split(train, train['label'])):\n",
    "        train.loc[v_idx, 'fold']  = n_fold\n",
    "    train['id'] = [x for x in range(len(train))]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1644155738450,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "7eyie7FFxSG8"
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  scheduler\n",
    "# ------------------------\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    target_lst = []\n",
    "    pred_lst = []\n",
    "    logit = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        ids  = data['ids'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "        tokentype = data['token_type_ids'].to(device)\n",
    "        target = data['targets'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "                    output = output[0]\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)#.squeeze(0)\n",
    "                loss = loss_fn(output, target)\n",
    "            \n",
    "            val_loss += loss\n",
    "            target_lst.extend(target.detach().cpu().numpy())\n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "        val_mean_loss = val_loss / len(valid_loader)\n",
    "        validation_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "        validation_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "        \n",
    "\n",
    "    return val_mean_loss, validation_score, validation_acc, logit\n",
    "\n",
    "def do_predict(net, valid_loader):\n",
    "    \n",
    "    val_loss = 0\n",
    "    pred_lst = []\n",
    "    logit=[]\n",
    "    net.eval()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        ids  = data['ids'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "        tokentype = data['token_type_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(ids, mask)[0]\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)\n",
    "             \n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "    return pred_lst,logit\n",
    "\n",
    "def run_train(folds=3):\n",
    "    out_dir = args.dir_+ f'/fold{args.fold}/{args.exp_name}/'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # load dataset\n",
    "    train, test = load_data()    \n",
    "    with open(f'./data/{args.pt}/train_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(f'./data/{args.pt}/test_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "        test_data = pickle.load(f)    \n",
    "    \n",
    "    # split fold\n",
    "    for n_fold in range(5):\n",
    "        print(n_fold)\n",
    "        if n_fold != folds:\n",
    "            print(f'{n_fold} fold pass'+'\\n')\n",
    "            continue\n",
    "            \n",
    "        if args.debug:\n",
    "            train = train.sample(1000).copy()\n",
    "            \n",
    "        print(n_fold)\n",
    "        \n",
    "        trn_idx = train[train['fold']!=n_fold]['id'].values\n",
    "        val_idx = train[train['fold']==n_fold]['id'].values\n",
    "    \n",
    "\n",
    "        train_dict = {'input_ids' : train_data['input_ids'][trn_idx] , 'attention_mask' : train_data['attention_mask'][trn_idx] , \n",
    "                      'token_type_ids' : train_data['token_type_ids'][trn_idx], 'targets' : train_data['targets'][trn_idx]}\n",
    "        val_dict = {'input_ids' : train_data['input_ids'][val_idx] , 'attention_mask' : train_data['attention_mask'][val_idx] , \n",
    "                      'token_type_ids' : train_data['token_type_ids'][val_idx], 'targets' : train_data['targets'][val_idx]}\n",
    "\n",
    "        ## dataset ------------------------------------\n",
    "        train_dataset = NliDataSet(data = train_dict)\n",
    "        valid_dataset = NliDataSet(data = val_dict)\n",
    "        trainloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
    "                                 num_workers=8, shuffle=True, pin_memory=True)\n",
    "        validloader = DataLoader(dataset=valid_dataset, batch_size=args.batch_size, \n",
    "                                 num_workers=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "        if 'xlm-roberta' in args.pt:\n",
    "            net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        \n",
    "        elif 'klue/roberta' in args.pt:\n",
    "            net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        else:\n",
    "            net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "\n",
    "        net.to(device)\n",
    "        if len(args.gpu)>1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "        # ------------------------\n",
    "        # loss\n",
    "        # ------------------------\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        optimizer = optim.Lookahead(optim.RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(trainloader)*args.epochs)\n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        start_timer = timer()\n",
    "        best_score = 0\n",
    "\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            train_loss = 0\n",
    "            valid_loss = 0\n",
    "\n",
    "            target_lst = []\n",
    "            pred_lst = []\n",
    "            lr = get_learning_rate(optimizer)\n",
    "            print(f'-------------------')\n",
    "            print(f'{epoch}epoch start')\n",
    "            print(f'-------------------'+'\\n')\n",
    "            print(f'learning rate : {lr : .6f}')\n",
    "            for t, data in enumerate(tqdm.tqdm(trainloader)):\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                ids  = data['ids'].to(device)\n",
    "                mask  = data['mask'].to(device)\n",
    "                tokentype = data['token_type_ids'].to(device)\n",
    "                target = data['targets'].to(device)\n",
    "\n",
    "                # ------------\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                if args.amp:\n",
    "                    with amp.autocast():\n",
    "                        # output\n",
    "                        output = net(ids, mask)\n",
    "                        output = output[0]\n",
    "\n",
    "                        # loss\n",
    "                        loss = loss_fn(output, target)\n",
    "                        train_loss += loss\n",
    "\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else:\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "                    train_loss += loss\n",
    "\n",
    "                    # update\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # for calculate f1 score\n",
    "                target_lst.extend(target.detach().cpu().numpy())\n",
    "                pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "            train_loss = train_loss / len(trainloader)\n",
    "            train_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "            train_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "\n",
    "            # validation\n",
    "            valid_loss, valid_score, valid_acc, _ = do_valid(net, validloader)\n",
    "\n",
    "\n",
    "            if valid_acc > best_score:\n",
    "                best_score = valid_acc\n",
    "                best_epoch = epoch\n",
    "                best_loss = valid_loss\n",
    "\n",
    "                torch.save(net.state_dict(), out_dir + f'/{folds}f_{epoch}e_{best_score:.4f}_s.pth')\n",
    "                print('best model saved'+'\\n')\n",
    "\n",
    "\n",
    "            print(f'train loss : {train_loss:.4f}, train f1 score : {train_score : .4f}, train acc : {train_acc : .4f}'+'\\n')\n",
    "            print(f'valid loss : {valid_loss:.4f}, valid f1 score : {valid_score : .4f}, valid acc : {valid_acc : .4f}'+'\\n')\n",
    "\n",
    "\n",
    "        print(f'best valid loss : {best_loss : .4f}'+'\\n')\n",
    "        print(f'best epoch : {best_epoch }'+'\\n')\n",
    "        print(f'best accuracy : {best_score : .4f}'+'\\n')\n",
    "        \n",
    "def run_predict(model_path):\n",
    "    ## dataset ------------------------------------\n",
    "    # load\n",
    "    with open(f'./data/{args.pt}/test_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "        test_dict = pickle.load(f)\n",
    "        \n",
    "    print('test load')\n",
    "    test_dataset = NliDataSet(data = test_dict, test=True)\n",
    "    testloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, \n",
    "                             num_workers=8, shuffle=False, pin_memory=True)\n",
    "    print('set testloader')\n",
    "    ## net ----------------------------------------\n",
    "    scaler = amp.GradScaler()\n",
    "    if 'xlm-roberta' in args.pt:\n",
    "        net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        \n",
    "    elif 'klue/roberta' in args.pt:\n",
    "        net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "    else:\n",
    "        net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    if len(args.gpu)>1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    f = torch.load(model_path)\n",
    "    net.load_state_dict(f, strict=True)  # True\n",
    "    print('load saved models')\n",
    "    # ------------------------\n",
    "    # validation\n",
    "    preds, logit = do_predict(net, testloader) #outputs\n",
    "           \n",
    "    print('complete predict')\n",
    "    \n",
    "    return preds, np.array(logit)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709,
     "referenced_widgets": [
      "7f2c0d3d34264e6ca05b3643e8de63e8",
      "bc5bc0135f5547cc921ea4dab74d7ae5",
      "001eba30d4be45b190c7ff47c41b2b4e",
      "87b45d1120754a8d90682eb6ee5a2b43",
      "d135467aa18647b284fcda02303293fd",
      "9628f91e1a0a4ed1bf72f4c6d633eb32",
      "82be25f2bb384d73b5f5d97d376b8c66",
      "4d592663a0404021bedaed03624de2f7",
      "31140f3b19744da5a893143ba86f7692",
      "78a9c5fd0a4f4d4a8c66770464d69288",
      "6d4a77754f7d4afe99bb02c82a5060ee"
     ]
    },
    "executionInfo": {
     "elapsed": 112961,
     "status": "error",
     "timestamp": 1644155851408,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "U2BclMLgTG10",
    "outputId": "063f91bc-896a-4039-e639-2d362965e98d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.49it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.6260, train f1 score :  0.7022, train acc :  0.7044\n",
      "\n",
      "valid loss : 0.3336, valid f1 score :  0.8854, valid acc :  0.8858\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.52it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2628, train f1 score :  0.9080, train acc :  0.9086\n",
      "\n",
      "valid loss : 0.2723, valid f1 score :  0.9062, valid acc :  0.9066\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:13<00:00,  6.46it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1502, train f1 score :  0.9483, train acc :  0.9486\n",
      "\n",
      "valid loss : 0.3078, valid f1 score :  0.9122, valid acc :  0.9128\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.53it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0837, train f1 score :  0.9737, train acc :  0.9739\n",
      "\n",
      "valid loss : 0.3457, valid f1 score :  0.9124, valid acc :  0.9130\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.54it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0487, train f1 score :  0.9845, train acc :  0.9845\n",
      "\n",
      "valid loss : 0.3773, valid f1 score :  0.9118, valid acc :  0.9124\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0305, train f1 score :  0.9911, train acc :  0.9912\n",
      "\n",
      "valid loss : 0.4091, valid f1 score :  0.9158, valid acc :  0.9164\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0219, train f1 score :  0.9930, train acc :  0.9930\n",
      "\n",
      "valid loss : 0.4546, valid f1 score :  0.9120, valid acc :  0.9126\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0140, train f1 score :  0.9959, train acc :  0.9959\n",
      "\n",
      "valid loss : 0.4478, valid f1 score :  0.9184, valid acc :  0.9188\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:13<00:00,  6.48it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0111, train f1 score :  0.9968, train acc :  0.9968\n",
      "\n",
      "valid loss : 0.4679, valid f1 score :  0.9197, valid acc :  0.9200\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.51it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0074, train f1 score :  0.9977, train acc :  0.9977\n",
      "\n",
      "valid loss : 0.4691, valid f1 score :  0.9200, valid acc :  0.9204\n",
      "\n",
      "best valid loss :  0.4691\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9204\n",
      "\n",
      "1\n",
      "1 fold pass\n",
      "\n",
      "2\n",
      "2 fold pass\n",
      "\n",
      "3\n",
      "3 fold pass\n",
      "\n",
      "4\n",
      "4 fold pass\n",
      "\n",
      "0\n",
      "0 fold pass\n",
      "\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.56it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.6054, train f1 score :  0.7151, train acc :  0.7166\n",
      "\n",
      "valid loss : 0.3245, valid f1 score :  0.8850, valid acc :  0.8862\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2590, train f1 score :  0.9069, train acc :  0.9075\n",
      "\n",
      "valid loss : 0.2701, valid f1 score :  0.9076, valid acc :  0.9082\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1448, train f1 score :  0.9515, train acc :  0.9518\n",
      "\n",
      "valid loss : 0.3061, valid f1 score :  0.9098, valid acc :  0.9102\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:13<00:00,  6.47it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0794, train f1 score :  0.9742, train acc :  0.9744\n",
      "\n",
      "valid loss : 0.3554, valid f1 score :  0.9122, valid acc :  0.9126\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.49it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0522, train f1 score :  0.9840, train acc :  0.9841\n",
      "\n",
      "valid loss : 0.3566, valid f1 score :  0.9141, valid acc :  0.9146\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.50it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0326, train f1 score :  0.9901, train acc :  0.9902\n",
      "\n",
      "valid loss : 0.4108, valid f1 score :  0.9129, valid acc :  0.9134\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.50it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0221, train f1 score :  0.9940, train acc :  0.9940\n",
      "\n",
      "valid loss : 0.4328, valid f1 score :  0.9133, valid acc :  0.9140\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0127, train f1 score :  0.9963, train acc :  0.9963\n",
      "\n",
      "valid loss : 0.4797, valid f1 score :  0.9125, valid acc :  0.9130\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0104, train f1 score :  0.9971, train acc :  0.9971\n",
      "\n",
      "valid loss : 0.4979, valid f1 score :  0.9124, valid acc :  0.9128\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.54it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0079, train f1 score :  0.9982, train acc :  0.9982\n",
      "\n",
      "valid loss : 0.4949, valid f1 score :  0.9136, valid acc :  0.9140\n",
      "\n",
      "best valid loss :  0.3566\n",
      "\n",
      "best epoch : 5\n",
      "\n",
      "best accuracy :  0.9146\n",
      "\n",
      "2\n",
      "2 fold pass\n",
      "\n",
      "3\n",
      "3 fold pass\n",
      "\n",
      "4\n",
      "4 fold pass\n",
      "\n",
      "0\n",
      "0 fold pass\n",
      "\n",
      "1\n",
      "1 fold pass\n",
      "\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:15<00:00,  6.40it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.6401, train f1 score :  0.6957, train acc :  0.6981\n",
      "\n",
      "valid loss : 0.3429, valid f1 score :  0.8761, valid acc :  0.8768\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.51it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2650, train f1 score :  0.9077, train acc :  0.9083\n",
      "\n",
      "valid loss : 0.3164, valid f1 score :  0.8903, valid acc :  0.8904\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1510, train f1 score :  0.9481, train acc :  0.9484\n",
      "\n",
      "valid loss : 0.2966, valid f1 score :  0.8998, valid acc :  0.9006\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.50it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0856, train f1 score :  0.9731, train acc :  0.9732\n",
      "\n",
      "valid loss : 0.3570, valid f1 score :  0.9059, valid acc :  0.9064\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.51it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0502, train f1 score :  0.9840, train acc :  0.9841\n",
      "\n",
      "valid loss : 0.4094, valid f1 score :  0.9027, valid acc :  0.9030\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:15<00:00,  6.41it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0290, train f1 score :  0.9915, train acc :  0.9916\n",
      "\n",
      "valid loss : 0.4545, valid f1 score :  0.9045, valid acc :  0.9050\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.54it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0216, train f1 score :  0.9932, train acc :  0.9933\n",
      "\n",
      "valid loss : 0.4943, valid f1 score :  0.9021, valid acc :  0.9028\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.55it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0137, train f1 score :  0.9963, train acc :  0.9963\n",
      "\n",
      "valid loss : 0.4730, valid f1 score :  0.9103, valid acc :  0.9108\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.56it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0093, train f1 score :  0.9975, train acc :  0.9975\n",
      "\n",
      "valid loss : 0.5281, valid f1 score :  0.9069, valid acc :  0.9074\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.56it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0075, train f1 score :  0.9982, train acc :  0.9982\n",
      "\n",
      "valid loss : 0.5154, valid f1 score :  0.9076, valid acc :  0.9082\n",
      "\n",
      "best valid loss :  0.4730\n",
      "\n",
      "best epoch : 8\n",
      "\n",
      "best accuracy :  0.9108\n",
      "\n",
      "3\n",
      "3 fold pass\n",
      "\n",
      "4\n",
      "4 fold pass\n",
      "\n",
      "0\n",
      "0 fold pass\n",
      "\n",
      "1\n",
      "1 fold pass\n",
      "\n",
      "2\n",
      "2 fold pass\n",
      "\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:15<00:00,  6.39it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.6224, train f1 score :  0.7132, train acc :  0.7158\n",
      "\n",
      "valid loss : 0.3480, valid f1 score :  0.8762, valid acc :  0.8772\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:16<00:00,  6.35it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2566, train f1 score :  0.9080, train acc :  0.9086\n",
      "\n",
      "valid loss : 0.2985, valid f1 score :  0.8955, valid acc :  0.8962\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.51it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1461, train f1 score :  0.9521, train acc :  0.9524\n",
      "\n",
      "valid loss : 0.3606, valid f1 score :  0.8966, valid acc :  0.8974\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:21<00:00,  6.22it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0852, train f1 score :  0.9723, train acc :  0.9725\n",
      "\n",
      "valid loss : 0.3771, valid f1 score :  0.8965, valid acc :  0.8974\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.53it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0499, train f1 score :  0.9839, train acc :  0.9840\n",
      "\n",
      "valid loss : 0.4448, valid f1 score :  0.9013, valid acc :  0.9020\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.49it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0337, train f1 score :  0.9900, train acc :  0.9900\n",
      "\n",
      "valid loss : 0.4484, valid f1 score :  0.9019, valid acc :  0.9026\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.54it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0202, train f1 score :  0.9943, train acc :  0.9943\n",
      "\n",
      "valid loss : 0.4838, valid f1 score :  0.9023, valid acc :  0.9030\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.54it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0146, train f1 score :  0.9951, train acc :  0.9951\n",
      "\n",
      "valid loss : 0.5214, valid f1 score :  0.9009, valid acc :  0.9018\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.52it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0101, train f1 score :  0.9974, train acc :  0.9974\n",
      "\n",
      "valid loss : 0.5181, valid f1 score :  0.9046, valid acc :  0.9054\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.51it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0074, train f1 score :  0.9982, train acc :  0.9982\n",
      "\n",
      "valid loss : 0.5200, valid f1 score :  0.9053, valid acc :  0.9060\n",
      "\n",
      "best valid loss :  0.5200\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9060\n",
      "\n",
      "4\n",
      "4 fold pass\n",
      "\n",
      "0\n",
      "0 fold pass\n",
      "\n",
      "1\n",
      "1 fold pass\n",
      "\n",
      "2\n",
      "2 fold pass\n",
      "\n",
      "3\n",
      "3 fold pass\n",
      "\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.54it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.6201, train f1 score :  0.7141, train acc :  0.7161\n",
      "\n",
      "valid loss : 0.3435, valid f1 score :  0.8720, valid acc :  0.8720\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.57it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2660, train f1 score :  0.9077, train acc :  0.9082\n",
      "\n",
      "valid loss : 0.2821, valid f1 score :  0.9004, valid acc :  0.9012\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.51it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1567, train f1 score :  0.9485, train acc :  0.9488\n",
      "\n",
      "valid loss : 0.2907, valid f1 score :  0.9024, valid acc :  0.9032\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.56it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0919, train f1 score :  0.9699, train acc :  0.9701\n",
      "\n",
      "valid loss : 0.3533, valid f1 score :  0.9047, valid acc :  0.9060\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.56it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0562, train f1 score :  0.9830, train acc :  0.9831\n",
      "\n",
      "valid loss : 0.4007, valid f1 score :  0.9065, valid acc :  0.9076\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.52it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0324, train f1 score :  0.9895, train acc :  0.9895\n",
      "\n",
      "valid loss : 0.4632, valid f1 score :  0.9034, valid acc :  0.9046\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:10<00:00,  6.57it/s]\n",
      "100%|██████████| 313/313 [00:15<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0246, train f1 score :  0.9928, train acc :  0.9928\n",
      "\n",
      "valid loss : 0.4447, valid f1 score :  0.9113, valid acc :  0.9122\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.52it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0158, train f1 score :  0.9953, train acc :  0.9953\n",
      "\n",
      "valid loss : 0.4618, valid f1 score :  0.9121, valid acc :  0.9132\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:11<00:00,  6.52it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0105, train f1 score :  0.9973, train acc :  0.9973\n",
      "\n",
      "valid loss : 0.4628, valid f1 score :  0.9137, valid acc :  0.9146\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:12<00:00,  6.50it/s]\n",
      "100%|██████████| 313/313 [00:14<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0097, train f1 score :  0.9973, train acc :  0.9973\n",
      "\n",
      "valid loss : 0.4640, valid f1 score :  0.9140, valid acc :  0.9148\n",
      "\n",
      "best valid loss :  0.4640\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5fold 전용\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args.exp_name = str(args.pt) + '_' + str(args.max_len)\n",
    "    \n",
    "    for i in [0,1,2,3,4]: # 5fold\n",
    "        run_train(folds=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    final_logit=0\n",
    "    \n",
    "    _, logit1 = run_predict(\"./saved_models/fold5/klue/roberta-large_100/0f_10e_0.9204_s.pth\")\n",
    "    _, logit2 = run_predict(\"./saved_models/fold5/klue/roberta-large_100/1f_5e_0.9146_s.pth\")\n",
    "    _, logit3 = run_predict(\"./saved_models/fold5/klue/roberta-large_100/2f_8e_0.9108_s.pth\")\n",
    "    _, logit4 = run_predict(\"./saved_models/fold5/klue/roberta-large_100/3f_9e_0.9054_s.pth\")\n",
    "    _, logit5 = run_predict(\"./saved_models/fold5/klue/roberta-large_100/4f_10e_0.9148_s.pth\")\n",
    "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
    "    \n",
    "    \n",
    "    return final_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 20.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 20.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_logit = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1        neutral\n",
      "2         2     entailment\n",
      "3         3  contradiction\n",
      "4         4  contradiction\n",
      "...     ...            ...\n",
      "1661   1661        neutral\n",
      "1662   1662        neutral\n",
      "1663   1663        neutral\n",
      "1664   1664        neutral\n",
      "1665   1665        neutral\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "out = [list(label_dict.keys())[_] for _ in final_logit.argmax(1)]\n",
    "\n",
    "sub['label'] = out\n",
    "print(sub)\n",
    "# preds\n",
    "sub.to_csv(f'./submission/final_submission_roberta-large.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMH1FJMRvE67Iv93Ol6JIih",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "nli_ver0.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001eba30d4be45b190c7ff47c41b2b4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82be25f2bb384d73b5f5d97d376b8c66",
      "placeholder": "​",
      "style": "IPY_MODEL_9628f91e1a0a4ed1bf72f4c6d633eb32",
      "value": "Downloading: 100%"
     }
    },
    "31140f3b19744da5a893143ba86f7692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d592663a0404021bedaed03624de2f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6d4a77754f7d4afe99bb02c82a5060ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78a9c5fd0a4f4d4a8c66770464d69288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f2c0d3d34264e6ca05b3643e8de63e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_001eba30d4be45b190c7ff47c41b2b4e",
       "IPY_MODEL_87b45d1120754a8d90682eb6ee5a2b43",
       "IPY_MODEL_d135467aa18647b284fcda02303293fd"
      ],
      "layout": "IPY_MODEL_bc5bc0135f5547cc921ea4dab74d7ae5"
     }
    },
    "82be25f2bb384d73b5f5d97d376b8c66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87b45d1120754a8d90682eb6ee5a2b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31140f3b19744da5a893143ba86f7692",
      "max": 442560877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4d592663a0404021bedaed03624de2f7",
      "value": 442560877
     }
    },
    "9628f91e1a0a4ed1bf72f4c6d633eb32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc5bc0135f5547cc921ea4dab74d7ae5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d135467aa18647b284fcda02303293fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d4a77754f7d4afe99bb02c82a5060ee",
      "placeholder": "​",
      "style": "IPY_MODEL_78a9c5fd0a4f4d4a8c66770464d69288",
      "value": " 422M/422M [00:07&lt;00:00, 61.3MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
