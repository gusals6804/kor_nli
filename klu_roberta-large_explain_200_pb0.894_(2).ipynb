{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ LIBRARY -------#\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
    "#\n",
    "\n",
    "import math\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch_optimizer as optim\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "#import time\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# transformer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# class args\n",
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    amp = True\n",
    "    gpu = '0'\n",
    "    \n",
    "    epochs=20\n",
    "    batch_size=10\n",
    "    weight_decay=0.0\n",
    "    n_fold=5\n",
    "    fold=5 # [0, 1, 2, 3, 4] # 원래는 3\n",
    "    patience = 7\n",
    "    \n",
    "    exp_name = 'experiment_name_folder'\n",
    "    dir_ = f'./saved_models/'\n",
    "    pt = 'xlm-roberta-large'\n",
    "    max_len = 200\n",
    "    \n",
    "    start_lr = 2e-5#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    lamb = 1.0\n",
    "    # ---- Dataset ---- #\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=222\n",
    "    scheduler = None#'get_linear_schedule_with_warmup'\n",
    "\n",
    "\n",
    "data_dir = './'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
    "\n",
    "set_seeds(seed=args.seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/new_train_data.csv\")\n",
    "test = pd.read_csv(\"data/test_data.csv\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction' 'entailment' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(train[\"label\"]))\n",
    "\n",
    "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(train.label):\n",
    "    train.label[i] = label_dict[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>2995</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>2996</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>2997</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선은 흔히 비자림로라고 불린다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>2998</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>비흡연자는 발코니 있는 방이 필요없습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>2999</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>흡연하려면 발코니 있는 방을 선택하면 됩니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            premise  \\\n",
       "0          0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1          1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2          2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3          3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4          4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "...      ...                                                ...   \n",
       "27993   2995  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27994   2996  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27995   2997  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27996   2998                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "27997   2999                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "\n",
       "                                    hypothesis label  \n",
       "0                               씨름의 여자들의 놀이이다.     1  \n",
       "1                             자작극을 벌인 이는 3명이다.     1  \n",
       "2      예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     0  \n",
       "3                            원주민들은 종합대책에 만족했다.     2  \n",
       "4           이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.     2  \n",
       "...                                        ...   ...  \n",
       "27993       지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.     1  \n",
       "27994       지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.     2  \n",
       "27995              지방도 제1112호선은 흔히 비자림로라고 불린다.     0  \n",
       "27996                  비흡연자는 발코니 있는 방이 필요없습니다.     2  \n",
       "27997                흡연하려면 발코니 있는 방을 선택하면 됩니다.     0  \n",
       "\n",
       "[27998 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_to_max_length(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n",
    "    List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    pad to maximum length of this batch\n",
    "    Args:\n",
    "        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n",
    "        max_len: specify max length\n",
    "        fill_values: specify filled values of each field\n",
    "    Returns:\n",
    "        output: list of field batched data, which shape is [batch, max_length]\n",
    "    \"\"\"\n",
    "    # [batch, num_fields]\n",
    "    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n",
    "    batch_size, num_fields = lengths.shape\n",
    "    fill_values = fill_values or [0.0] * num_fields\n",
    "    # [num_fields]\n",
    "    max_lengths = lengths.max(axis=0)\n",
    "    if max_len:\n",
    "        assert max_lengths.max() <= max_len\n",
    "        max_lengths = np.ones_like(max_lengths) * max_len\n",
    "\n",
    "    output = [torch.full([batch_size, max_lengths[field_idx]],\n",
    "                         fill_value=fill_values[field_idx],\n",
    "                         dtype=batch[0][field_idx].dtype)\n",
    "              for field_idx in range(num_fields)]\n",
    "    for sample_idx in range(batch_size):\n",
    "        for field_idx in range(num_fields):\n",
    "            # seq_length\n",
    "            data = batch[sample_idx][field_idx]\n",
    "            output[field_idx][sample_idx][: data.shape[0]] = data\n",
    "    # generate span_index and span_mask\n",
    "    max_sentence_length = max_lengths[0]\n",
    "    start_indexs = []\n",
    "    end_indexs = []\n",
    "    for i in range(1, max_sentence_length - 1):\n",
    "        for j in range(i, max_sentence_length - 1):\n",
    "            # # span大小为10\n",
    "            # if j - i > 10:\n",
    "            #     continue\n",
    "            start_indexs.append(i)\n",
    "            end_indexs.append(j)\n",
    "    # generate span mask\n",
    "    span_masks = []\n",
    "    for input_ids, label, length in batch:\n",
    "        span_mask = []\n",
    "        middle_index = input_ids.tolist().index(2)\n",
    "        for start_index, end_index in zip(start_indexs, end_indexs):\n",
    "            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n",
    "                start_index > middle_index or end_index < middle_index):\n",
    "                span_mask.append(0)\n",
    "            else:\n",
    "                span_mask.append(1e6)\n",
    "        span_masks.append(span_mask)\n",
    "    # add to output\n",
    "    output.append(torch.LongTensor(start_indexs))\n",
    "    output.append(torch.LongTensor(end_indexs))\n",
    "    output.append(torch.LongTensor(span_masks))\n",
    "    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_to_max_length_test(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n",
    "    List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    pad to maximum length of this batch\n",
    "    Args:\n",
    "        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n",
    "        max_len: specify max length\n",
    "        fill_values: specify filled values of each field\n",
    "    Returns:\n",
    "        output: list of field batched data, which shape is [batch, max_length]\n",
    "    \"\"\"\n",
    "    # [batch, num_fields]\n",
    "    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n",
    "    batch_size, num_fields = lengths.shape\n",
    "    fill_values = fill_values or [0.0] * num_fields\n",
    "    # [num_fields]\n",
    "    max_lengths = lengths.max(axis=0)\n",
    "    if max_len:\n",
    "        assert max_lengths.max() <= max_len\n",
    "        max_lengths = np.ones_like(max_lengths) * max_len\n",
    "\n",
    "    output = [torch.full([batch_size, max_lengths[field_idx]],\n",
    "                         fill_value=fill_values[field_idx],\n",
    "                         dtype=batch[0][field_idx].dtype)\n",
    "              for field_idx in range(num_fields)]\n",
    "    for sample_idx in range(batch_size):\n",
    "        for field_idx in range(num_fields):\n",
    "            # seq_length\n",
    "            data = batch[sample_idx][field_idx]\n",
    "            output[field_idx][sample_idx][: data.shape[0]] = data\n",
    "    # generate span_index and span_mask\n",
    "    max_sentence_length = max_lengths[0]\n",
    "    start_indexs = []\n",
    "    end_indexs = []\n",
    "    for i in range(1, max_sentence_length - 1):\n",
    "        for j in range(i, max_sentence_length - 1):\n",
    "            # # span大小为10\n",
    "            # if j - i > 10:\n",
    "            #     continue\n",
    "            start_indexs.append(i)\n",
    "            end_indexs.append(j)\n",
    "    # generate span mask\n",
    "    span_masks = []\n",
    "    for input_ids, length in batch:\n",
    "        span_mask = []\n",
    "        middle_index = input_ids.tolist().index(2)\n",
    "        for start_index, end_index in zip(start_indexs, end_indexs):\n",
    "            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n",
    "                start_index > middle_index or end_index < middle_index):\n",
    "                span_mask.append(0)\n",
    "            else:\n",
    "                span_mask.append(1e6)\n",
    "        span_masks.append(span_mask)\n",
    "    # add to output\n",
    "    output.append(torch.LongTensor(start_indexs))\n",
    "    output.append(torch.LongTensor(end_indexs))\n",
    "    output.append(torch.LongTensor(span_masks))\n",
    "    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='xlm-roberta-large', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(args.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, bert_path, max_length):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.result = []\n",
    "\n",
    "        train=data[['premise', 'hypothesis', 'label']]\n",
    "\n",
    "#         train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "#         train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "        \n",
    "        for train_premise, train_hypothesis, train_label in tqdm.tqdm(zip(train['premise'], train['hypothesis'], train['label'])):\n",
    "                    self.result.append((train_premise, train_hypothesis, train_label))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_1, sentence_2, label = self.result[idx]\n",
    "        # remove .\n",
    "        if sentence_1.endswith(\".\"):\n",
    "            sentence_1 = sentence_1[:-1]\n",
    "        if sentence_2.endswith(\".\"):\n",
    "            sentence_2 = sentence_2[:-1]\n",
    "        sentence_1_input_ids = self.tokenizer.encode(sentence_1, add_special_tokens=False)\n",
    "        sentence_2_input_ids = self.tokenizer.encode(sentence_2, add_special_tokens=False)\n",
    "        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n",
    "        if len(input_ids) > self.max_length - 2:\n",
    "            input_ids = input_ids[:self.max_length - 2]\n",
    "        # convert list to tensor\n",
    "        length = torch.LongTensor([len(input_ids) + 2])\n",
    "        input_ids = torch.LongTensor([0] + input_ids + [2])\n",
    "        label = torch.LongTensor([label])\n",
    "        \n",
    "        return input_ids, label, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self, data, bert_path, max_length: int = 512):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.result = []\n",
    "        \n",
    "        test=data[['premise', 'hypothesis']]\n",
    "    \n",
    "#         test['premise'] = test['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "#         test['hypothesis'] = test['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "    \n",
    "        for test_premise, test_hypothesis in tqdm.tqdm(zip(test['premise'], test['hypothesis'])):\n",
    "                    self.result.append((test_premise, test_hypothesis))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_1, sentence_2 = self.result[idx]\n",
    "        # remove .\n",
    "        if sentence_1.endswith(\".\"):\n",
    "            sentence_1 = sentence_1[:-1]\n",
    "        if sentence_2.endswith(\".\"):\n",
    "            sentence_2 = sentence_2[:-1]\n",
    "        sentence_1_input_ids = self.tokenizer.encode(sentence_1, add_special_tokens=False)\n",
    "        sentence_2_input_ids = self.tokenizer.encode(sentence_2, add_special_tokens=False)\n",
    "        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n",
    "        if len(input_ids) > self.max_length - 2:\n",
    "            input_ids = input_ids[:self.max_length - 2]\n",
    "        # convert list to tensor\n",
    "        length = torch.LongTensor([len(input_ids) + 2])\n",
    "        input_ids = torch.LongTensor([0] + input_ids + [2])\n",
    "        \n",
    "        return input_ids, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test():\n",
    "    \n",
    "    dataset = NLIDataset(data=train[:1], bert_path=args.pt, max_length=args.max_len)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=10,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0])\n",
    "    )\n",
    "    for input_ids, label, length, start_index, end_index, span_mask in dataloader:\n",
    "        print(input_ids.shape, input_ids)\n",
    "        print(start_index.shape, start_index)\n",
    "        print(end_index.shape, end_index)\n",
    "        print(span_mask.shape, span_mask)\n",
    "        print(label.view(-1).shape, label)\n",
    "        print()\n",
    "        \n",
    "    for t, data in enumerate(tqdm.tqdm(dataloader)):\n",
    "        print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 8924.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 59]) tensor([[     0, 105051,  28913,    697,  11031,   1077, 128161,  84802,   3626,\n",
      "           1963,  25436, 105646, 119686,  64757,  17862, 223713,      6, 145726,\n",
      "          67520,      4,   6705,   2680,  16632,  11619,   2905,   7593,      6,\n",
      "         154848,   1077,  51851,  27815,    993,  36372, 102102,  16632,      6,\n",
      "         202577,   1180, 209750,  77442,  66127,   1291,  64730,  32685,      6,\n",
      "          23854,  14413,    769,  15710,      2, 105051,  28913,    367,  52340,\n",
      "          17862,      6, 145726,   5769,      2]])\n",
      "torch.Size([1653]) tensor([ 1,  1,  1,  ..., 56, 56, 57])\n",
      "torch.Size([1653]) tensor([ 1,  2,  3,  ..., 56, 57, 57])\n",
      "torch.Size([1, 1653]) tensor([[0, 0, 0,  ..., 0, 0, 0]])\n",
      "torch.Size([1]) tensor([[1]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 174.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0, 105051,  28913,    697,  11031,   1077, 128161,  84802,   3626,\n",
      "           1963,  25436, 105646, 119686,  64757,  17862, 223713,      6, 145726,\n",
      "          67520,      4,   6705,   2680,  16632,  11619,   2905,   7593,      6,\n",
      "         154848,   1077,  51851,  27815,    993,  36372, 102102,  16632,      6,\n",
      "         202577,   1180, 209750,  77442,  66127,   1291,  64730,  32685,      6,\n",
      "          23854,  14413,    769,  15710,      2, 105051,  28913,    367,  52340,\n",
      "          17862,      6, 145726,   5769,      2]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableModel(nn.Module):\n",
    "    def __init__(self, bert_dir):\n",
    "        super().__init__()\n",
    "        self.bert_config = AutoConfig.from_pretrained(bert_dir, output_hidden_states=False)\n",
    "        self.intermediate = AutoModel.from_pretrained(bert_dir, return_dict=False)\n",
    "        self.span_info_collect = SICModel(self.bert_config.hidden_size)\n",
    "        self.interpretation = InterpretationModel(self.bert_config.hidden_size)\n",
    "        self.output = nn.Linear(self.bert_config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n",
    "        # generate mask\n",
    "        attention_mask = (input_ids != 1).long()\n",
    "        # intermediate layer\n",
    "        hidden_states, first_token = self.intermediate(input_ids, attention_mask=attention_mask)  # output.shape = (bs, length, hidden_size)\n",
    "        # span info collecting layer(SIC)\n",
    "        h_ij = self.span_info_collect(hidden_states, start_indexs, end_indexs)\n",
    "        # interpretation layer\n",
    "        H, a_ij = self.interpretation(h_ij, span_masks)\n",
    "        # output layer\n",
    "        out = self.output(H)\n",
    "        return out, a_ij\n",
    "\n",
    "\n",
    "class SICModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, start_indexs, end_indexs):\n",
    "        W1_h = self.W_1(hidden_states)  # (bs, length, hidden_size)\n",
    "        W2_h = self.W_2(hidden_states)\n",
    "        W3_h = self.W_3(hidden_states)\n",
    "        W4_h = self.W_4(hidden_states)\n",
    "\n",
    "        W1_hi_emb = torch.index_select(W1_h, 1, start_indexs)  # (bs, span_num, hidden_size)\n",
    "        W2_hj_emb = torch.index_select(W2_h, 1, end_indexs)\n",
    "        W3_hi_start_emb = torch.index_select(W3_h, 1, start_indexs)\n",
    "        W3_hi_end_emb = torch.index_select(W3_h, 1, end_indexs)\n",
    "        W4_hj_start_emb = torch.index_select(W4_h, 1, start_indexs)\n",
    "        W4_hj_end_emb = torch.index_select(W4_h, 1, end_indexs)\n",
    "\n",
    "        # [w1*hi, w2*hj, w3(hi-hj), w4(hi⊗hj)]\n",
    "        span = W1_hi_emb + W2_hj_emb + (W3_hi_start_emb - W3_hi_end_emb) + torch.mul(W4_hj_start_emb, W4_hj_end_emb)\n",
    "        h_ij = torch.tanh(span)\n",
    "        return h_ij\n",
    "\n",
    "\n",
    "class InterpretationModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.h_t = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, h_ij, span_masks):\n",
    "        o_ij = self.h_t(h_ij).squeeze(-1)  # (ba, span_num)\n",
    "        # mask illegal span\n",
    "        o_ij = o_ij - span_masks\n",
    "        # normalize all a_ij, a_ij sum = 1\n",
    "        a_ij = nn.functional.softmax(o_ij, dim=1)\n",
    "        # weight average span representation to get H\n",
    "        H = (a_ij.unsqueeze(-1) * h_ij).sum(dim=1)  # (bs, hidden_size)\n",
    "        return H, a_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - util - #\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "def load_data():\n",
    "    train=pd.read_csv('data/new_train_data.csv')\n",
    "    test=pd.read_csv('data/test_data.csv')\n",
    "    \n",
    "    #\n",
    "    train=train[['premise', 'hypothesis', 'label']]\n",
    "    test=test[['premise', 'hypothesis']]\n",
    "    \n",
    "    #\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    train['fold'] = -1\n",
    "    for n_fold, (_,v_idx) in enumerate(skf.split(train, train['label'])):\n",
    "        train.loc[v_idx, 'fold']  = n_fold\n",
    "    train['id'] = [x for x in range(len(train))]\n",
    "    \n",
    "    for i, text in enumerate(train.label):\n",
    "        train.label[i] = label_dict[text]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  scheduler\n",
    "# ------------------------\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    target_lst = []\n",
    "    pred_lst = []\n",
    "    logit = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "\n",
    "        # (input_ids, labels, length, start_indexs, end_indexs, span_masks)\n",
    "        input_ids  = data[0].to(device)\n",
    "        start_index  = data[3].to(device)\n",
    "        end_index = data[4].to(device)\n",
    "        span_mask = data[5].to(device)\n",
    "        target = data[1].to(device).view(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output, a_ij = net(input_ids, start_index, end_index, span_mask)\n",
    "#                     output = output[0]\n",
    "\n",
    "                    # loss\n",
    "                    ce_loss  = loss_fn(output, target)\n",
    "                    reg_loss = args.lamb * a_ij.pow(2).sum(dim=1).mean()\n",
    "                    loss = ce_loss + reg_loss\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)#.squeeze(0)\n",
    "                loss = loss_fn(output, target)\n",
    "            \n",
    "            val_loss += loss\n",
    "            target_lst.extend(target.detach().cpu().numpy())\n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "        val_mean_loss = val_loss / len(valid_loader)\n",
    "        validation_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "        validation_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "        \n",
    "\n",
    "    return val_mean_loss, validation_score, validation_acc, logit\n",
    "\n",
    "def do_predict(net, valid_loader):\n",
    "    \n",
    "    val_loss = 0\n",
    "    pred_lst = []\n",
    "    logit=[]\n",
    "    net.eval()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        \n",
    "        # (input_ids, length, start_indexs, end_indexs, span_masks)\n",
    "        input_ids  = data[0].to(device)\n",
    "        start_index  = data[2].to(device)\n",
    "        end_index = data[3].to(device)\n",
    "        span_mask = data[4].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(input_ids, start_index, end_index, span_mask)[0]\n",
    "\n",
    "            else:\n",
    "                output = net(input_ids, start_index, end_index, span_mask)\n",
    "             \n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "    return pred_lst,logit\n",
    "\n",
    "def run_train(folds=3):\n",
    "    out_dir = args.dir_+ f'/fold{args.fold}/{args.exp_name}/'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # load dataset\n",
    "    train, test = load_data()    \n",
    " \n",
    "    \n",
    "    # split fold\n",
    "    for n_fold in range(5):\n",
    "        \n",
    "        print(n_fold)\n",
    "        if n_fold != folds:\n",
    "            print(f'{n_fold} fold pass'+'\\n')\n",
    "            continue\n",
    "            \n",
    "        if args.debug:\n",
    "            train = train.sample(1000).copy()\n",
    "            \n",
    "        print(n_fold)\n",
    "        \n",
    "        trn_idx = train[train['fold']!=n_fold]['id'].values\n",
    "        val_idx = train[train['fold']==n_fold]['id'].values\n",
    "\n",
    "        ## dataset ------------------------------------\n",
    "        train_dataset = NLIDataset(data = train.iloc[trn_idx], bert_path=args.pt, max_length=args.max_len)\n",
    "        valid_dataset = NLIDataset(data = train.iloc[val_idx], bert_path=args.pt, max_length=args.max_len)\n",
    "        trainloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
    "                                 num_workers=8, shuffle=True, pin_memory=True, \n",
    "                                 collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]),\n",
    "                                drop_last=False)\n",
    "        validloader = DataLoader(dataset=valid_dataset, batch_size=args.batch_size, \n",
    "                                 num_workers=8, shuffle=False, pin_memory=True, \n",
    "                                 collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]),\n",
    "                                drop_last=False)\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "        net = ExplainableModel(args.pt)\n",
    "\n",
    "        net.to(device)\n",
    "        if len(args.gpu)>1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "        # ------------------------\n",
    "        # loss\n",
    "        # ------------------------\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "#         no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {\n",
    "#                 \"params\": [p for n, p in net.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#                 \"weight_decay\": args.weight_decay,\n",
    "#             },\n",
    "#             {\n",
    "#                 \"params\": [p for n, p in net.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#                 \"weight_decay\": 0.0,\n",
    "#             },\n",
    "#         ]\n",
    "#         optimizer = AdamW(optimizer_grouped_parameters,\n",
    "#                           betas=(0.9, 0.98),  # according to RoBERTa paper\n",
    "#                           lr=args.start_lr,\n",
    "#                           eps=1e-9)\n",
    "\n",
    "        optimizer = Lookahead(optim.RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
    "    \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(trainloader)*args.epochs)\n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        start_timer = timer()\n",
    "        best_score = 0\n",
    "        early_stopping = 0\n",
    "        \n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            train_loss = 0\n",
    "            valid_loss = 0\n",
    "\n",
    "            target_lst = []\n",
    "            pred_lst = []\n",
    "#             lr = get_learning_rate(optimizer)\n",
    "            print(f'-------------------')\n",
    "            print(f'{epoch}epoch start')\n",
    "            print(f'-------------------'+'\\n')\n",
    "#             print(f'learning rate : {lr : .6f}')\n",
    "            for t, data in enumerate(tqdm.tqdm(trainloader)):\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                input_ids  = data[0].to(device)\n",
    "                start_index  = data[3].to(device)\n",
    "                end_index = data[4].to(device)\n",
    "                span_mask = data[5].to(device)\n",
    "                target = data[1].to(device).view(-1)\n",
    "\n",
    "                # ------------\n",
    "#                 net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                if args.amp:\n",
    "                    with amp.autocast():\n",
    "                        # output\n",
    "                        output, a_ij = net(input_ids, start_index, end_index, span_mask)\n",
    "#                         output = output[0]\n",
    "\n",
    "                        # loss\n",
    "                        ce_loss  = loss_fn(output, target)\n",
    "                        reg_loss = args.lamb * a_ij.pow(2).sum(dim=1).mean()\n",
    "                        loss = ce_loss + reg_loss\n",
    "                        train_loss += loss\n",
    "\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else:\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "                    train_loss += loss\n",
    "\n",
    "                    # update\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # for calculate f1 score\n",
    "                target_lst.extend(target.detach().cpu().numpy())\n",
    "                pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "            train_loss = train_loss / len(trainloader)\n",
    "            train_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "            train_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "\n",
    "            # validation\n",
    "            valid_loss, valid_score, valid_acc, _ = do_valid(net, validloader)\n",
    "\n",
    "\n",
    "            if valid_acc > best_score:\n",
    "                best_score = valid_acc\n",
    "                best_epoch = epoch\n",
    "                best_loss = valid_loss\n",
    "\n",
    "                torch.save(net.state_dict(), out_dir + f'/{folds}f_explain.pth')\n",
    "                print('best model saved'+'\\n')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "\n",
    "            print(f'train loss : {train_loss:.4f}, train f1 score : {train_score : .4f}, train acc : {train_acc : .4f}'+'\\n')\n",
    "            print(f'valid loss : {valid_loss:.4f}, valid f1 score : {valid_score : .4f}, valid acc : {valid_acc : .4f}'+'\\n')\n",
    "\n",
    "\n",
    "        print(f'best valid loss : {best_loss : .4f}'+'\\n')\n",
    "        print(f'best epoch : {best_epoch }'+'\\n')\n",
    "        print(f'best accuracy : {best_score : .4f}'+'\\n')\n",
    "        \n",
    "def run_predict(model_path):\n",
    "    ## dataset ------------------------------------\n",
    "    # load\n",
    "        \n",
    "    train, test = load_data()\n",
    "    print('test load')\n",
    "\n",
    "    test_dataset = NLIDataset_test(data = test, bert_path=args.pt, max_length=args.max_len)\n",
    "    testloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, \n",
    "                             num_workers=8, shuffle=False, pin_memory=True, collate_fn=partial(collate_to_max_length_test, fill_values=[1, 0, 0]))\n",
    "    print('set testloader')\n",
    "    ## net ----------------------------------------\n",
    "    scaler = amp.GradScaler()\n",
    "    net = ExplainableModel(args.pt)\n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    if len(args.gpu)>1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    f = torch.load(model_path)\n",
    "    net.load_state_dict(f, strict=True)  # True\n",
    "    print('load saved models')\n",
    "    # ------------------------\n",
    "    # validation\n",
    "    preds, logit = do_predict(net, testloader) #outputs\n",
    "           \n",
    "    print('complete predict')\n",
    "    \n",
    "    return preds, np.array(logit)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"5fold 전용\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     for pt, max_len in zip(['klue/roberta-large'],[200]):\n",
    "        \n",
    "#         args.max_len = max_len\n",
    "#         args.pt = pt\n",
    "#         args.exp_name = str(args.pt) + '_' + str(args.max_len)\n",
    "    \n",
    "#         for i in [0,1,2,3,4]: # 5fold\n",
    "#             run_train(folds=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    final_logit=0\n",
    "    \n",
    "    args.pt = 'klue/roberta-large'\n",
    "    _, logit1 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/0f_explain.pth\")\n",
    "    _, logit2 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/1f_explain.pth\")\n",
    "    _, logit3 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/2f_explain.pth\")\n",
    "    _, logit4 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/3f_explain.pth\")\n",
    "    _, logit5 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/4f_explain.pth\")\n",
    "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
    "    \n",
    "#     args.pt = 'xlm-roberta-large'\n",
    "#     _, logit1 = run_predict(\"./saved_models/fold5/xlm-roberta-large_200/0f_explain.pth\")\n",
    "#     _, logit2 = run_predict(\"./saved_models/fold5/xlm-roberta-large_200/1f_explain.pth\")\n",
    "#     _, logit3 = run_predict(\"./saved_models/fold5/xlm-roberta-large_200/2f_explain.pth\")\n",
    "#     _, logit4 = run_predict(\"./saved_models/fold5/xlm-roberta-large_200/3f_explain.pth\")\n",
    "#     _, logit5 = run_predict(\"./saved_models/fold5/xlm-roberta-large_200/4f_explain.pth\")\n",
    "#     final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
    "    \n",
    "    \n",
    "    return final_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 725618.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:07<00:00, 20.94it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 961302.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:07<00:00, 21.47it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 866316.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:07<00:00, 21.58it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 914621.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:07<00:00, 21.94it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 797957.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:07<00:00, 21.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_logit = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.42070313,  6.121875  , -2.78007813],\n",
       "       [-1.14541016, -3.11367188,  4.28945312],\n",
       "       [ 4.20195313, -3.57070313, -0.59462891],\n",
       "       ...,\n",
       "       [-1.78173828, -3.45078125,  5.375     ],\n",
       "       [-1.95351563, -3.7       ,  5.8640625 ],\n",
       "       [-0.75383301, -0.95227051,  1.52275391]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./explain_npy', final_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_logit = np.load('./robert_npy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final_logit + robert_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1        neutral\n",
      "2         2     entailment\n",
      "3         3  contradiction\n",
      "4         4  contradiction\n",
      "...     ...            ...\n",
      "1661   1661        neutral\n",
      "1662   1662        neutral\n",
      "1663   1663        neutral\n",
      "1664   1664        neutral\n",
      "1665   1665        neutral\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "out = [list(label_dict.keys())[_] for _ in final.argmax(1)]\n",
    "\n",
    "sub['label'] = out\n",
    "print(sub)\n",
    "# preds\n",
    "sub.to_csv(f'./submission/final_submission_klue_roberta-large_explain_200_0216_(2).csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
