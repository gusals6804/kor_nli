{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1644155659595,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "y5joRz5dWOJJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3512,
     "status": "ok",
     "timestamp": 1644155666125,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "saedTdX9XVyl"
   },
   "outputs": [],
   "source": [
    "# ------ LIBRARY -------#\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "# torch\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
    "#\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch_optimizer as optim\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "#import time\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# transformer\n",
    "from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n",
    "from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n",
    "XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1644155666126,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "tu3duc0IbgYR"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/new_train_data.csv\")\n",
    "test = pd.read_csv(\"data/test_data.csv\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1644155666127,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "IOmwy-ZUb8K3",
    "outputId": "8217e17b-fe08-4f6b-f9e7-d3bcdb39160d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>2995</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>2996</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>2997</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선은 흔히 비자림로라고 불린다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>2998</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>비흡연자는 발코니 있는 방이 필요없습니다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>2999</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>흡연하려면 발코니 있는 방을 선택하면 됩니다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            premise  \\\n",
       "0          0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1          1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2          2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3          3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4          4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "...      ...                                                ...   \n",
       "27993   2995  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27994   2996  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27995   2997  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27996   2998                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "27997   2999                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "\n",
       "                                    hypothesis          label  \n",
       "0                               씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                             자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2      예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                            원주민들은 종합대책에 만족했다.        neutral  \n",
       "4           이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  \n",
       "...                                        ...            ...  \n",
       "27993       지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.  contradiction  \n",
       "27994       지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.        neutral  \n",
       "27995              지방도 제1112호선은 흔히 비자림로라고 불린다.     entailment  \n",
       "27996                  비흡연자는 발코니 있는 방이 필요없습니다.        neutral  \n",
       "27997                흡연하려면 발코니 있는 방을 선택하면 됩니다.     entailment  \n",
       "\n",
       "[27998 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise 최대 길이: 90\n",
      "hypothesis 최대 길이: 103\n",
      "premise 최대 길이: 90\n",
      "hypothesis 최대 길이: 75\n"
     ]
    }
   ],
   "source": [
    "print(\"premise 최대 길이:\", train['premise'].map(len).max())\n",
    "print(\"hypothesis 최대 길이:\", train['hypothesis'].map(len).max())\n",
    "\n",
    "print(\"premise 최대 길이:\", test['premise'].map(len).max())\n",
    "print(\"hypothesis 최대 길이:\", test['hypothesis'].map(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1644155676076,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "1SSkHU2_oWo0"
   },
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    # line = re.sub(r\"[^가-힣a-zA-Z0-9\\u4e00-\\u9fa5\\s]+\", \" \", line)\n",
    "    line = re.sub(r\"[^0-9]+\", \"\", line)\n",
    "#     line = \" \".join(filter(lambda word: not word.isdigit(), line.split()))\n",
    "#     line = re.sub(r\"\\s{2,}\", \" \", line).strip()\n",
    "    # line = line.lower()\n",
    "\n",
    "#     if len(line) >= 1 and line[1] == \" \":\n",
    "#         line = line[2:].strip()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1644155666515,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "HLpXmlVyb9K2",
    "outputId": "25b7235a-adce-4ee7-c368-8133949074f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# class args\n",
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    amp = True\n",
    "    gpu = '1'\n",
    "    \n",
    "    epochs=30\n",
    "    batch_size=32\n",
    "    weight_decay=1e-6\n",
    "    n_fold=1\n",
    "    fold=5 # [0, 1, 2, 3, 4] # 원래는 3\n",
    "    patience = 5\n",
    "    \n",
    "    exp_name = 'experiment_name_folder'\n",
    "    dir_ = f'./saved_models/'\n",
    "    pt = 'xlm-roberta-large'\n",
    "    max_len = 194\n",
    "    \n",
    "    start_lr = 2e-5#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=2021\n",
    "    scheduler = None#'get_linear_schedule_with_warmup'\n",
    "\n",
    "\n",
    "data_dir = './'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
    "\n",
    "set_seeds(seed=args.seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1644155666516,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "aF8L7RYKuOWR",
    "outputId": "ab47b0aa-4f34-4d0a-caf3-1b3ba3d66a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction' 'entailment' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(train[\"label\"]))\n",
    "\n",
    "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9285,
     "status": "ok",
     "timestamp": 1644155676074,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "cEoMqloPuurY",
    "outputId": "805bfcf7-1eb3-4ede-9489-53485dcbfcf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(train.label):\n",
    "    train.label[i] = label_dict[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1644155676075,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "8hBhBeAdvQwW",
    "outputId": "e8150837-b543-43de-ba1f-52ec6ab5d669",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>2995</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>2996</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>2997</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선은 흔히 비자림로라고 불린다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>2998</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>비흡연자는 발코니 있는 방이 필요없습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>2999</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>흡연하려면 발코니 있는 방을 선택하면 됩니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            premise  \\\n",
       "0          0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1          1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2          2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3          3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4          4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "...      ...                                                ...   \n",
       "27993   2995  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27994   2996  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27995   2997  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "27996   2998                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "27997   2999                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "\n",
       "                                    hypothesis label  \n",
       "0                               씨름의 여자들의 놀이이다.     1  \n",
       "1                             자작극을 벌인 이는 3명이다.     1  \n",
       "2      예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     0  \n",
       "3                            원주민들은 종합대책에 만족했다.     2  \n",
       "4           이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.     2  \n",
       "...                                        ...   ...  \n",
       "27993       지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.     1  \n",
       "27994       지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.     2  \n",
       "27995              지방도 제1112호선은 흔히 비자림로라고 불린다.     0  \n",
       "27996                  비흡연자는 발코니 있는 방이 필요없습니다.     2  \n",
       "27997                흡연하려면 발코니 있는 방을 선택하면 됩니다.     0  \n",
       "\n",
       "[27998 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619b2fb3557b468a84aa32a71250beb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7a3c595e6f4c8bad45cf1cc72bb1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759499b362ab43aca426fdb61feb9f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 105051, 28913, 697, 11031, 1077, 128161, 84802, 3626, 1963, 25436, 105646, 119686, 64757, 17862, 223713, 6, 145726, 67520, 4, 6705, 2680, 16632, 11619, 2905, 7593, 6, 154848, 1077, 51851, 27815, 993, 36372, 102102, 16632, 6, 202577, 1180, 209750, 77442, 66127, 1291, 64730, 32685, 6, 23854, 14413, 769, 15710, 5, 2, 2, 105051, 28913, 367, 52340, 17862, 6, 145726, 5769, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.pt, use_fast=True)\n",
    "tokenizer(train['premise'][0], train['hypothesis'][0], max_length=args.max_len, \n",
    "                                                pad_to_max_length=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1644155676076,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "y18oNP_jeegx"
   },
   "outputs": [],
   "source": [
    "def preprocessing_train(data):\n",
    "    \n",
    "    pt = args.pt#'monologg/kobert'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pt, use_fast=True)\n",
    "    \n",
    "    MAX_LEN = args.max_len\n",
    "    train=data[['premise', 'hypothesis', 'label']]\n",
    "\n",
    "    lines = []\n",
    "#     for _, row in tqdm.tqdm(train.iterrows()):\n",
    "#         line = preprocessing(row.premise)\n",
    "#         lines.append(line)\n",
    "#     train['premise_pre'] = lines\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(train.iterrows()):\n",
    "#         line = preprocessing(row.hypothesis)\n",
    "#         lines.append(line)\n",
    "#     train['hypothesis_pre'] = lines\n",
    "\n",
    "    input_ids =[]\n",
    "    attention_masks =[]\n",
    "    token_type_ids =[]\n",
    "    train_data_labels = []\n",
    "\n",
    "    for train_premise, train_hypothesis, train_label in tqdm.tqdm(zip(train['premise'], train['hypothesis'], train['label'])):\n",
    "        try:\n",
    "            train_sent = train_premise + \" \" + tokenizer.sep_token + \" \" + train_hypothesis\n",
    "            token= tokenizer(train_sent, max_length=args.max_len, \n",
    "                                                pad_to_max_length=True, return_token_type_ids=False)\n",
    "\n",
    "            input_ids.append(token['input_ids'])\n",
    "            attention_masks.append(token['attention_mask'])\n",
    "#             token_type_ids.append(token['token_type_ids'])\n",
    "            #########################################\n",
    "            train_data_labels.append(train_label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    train_input_ids=np.array(input_ids, dtype=int)\n",
    "    train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "#     train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "    ###########################################################\n",
    "    train_inputs=(train_input_ids, train_attention_masks)\n",
    "    train_labels=np.asarray(train_data_labels, dtype=np.int32)\n",
    "\n",
    "    # save\n",
    "    train_data = {}\n",
    "\n",
    "    train_data['input_ids'] = train_input_ids\n",
    "    train_data['attention_mask'] = train_attention_masks\n",
    "#     train_data['token_type_ids'] = train_token_type_ids\n",
    "    train_data['targets'] = np.asarray(train_data_labels, dtype=np.int32)\n",
    "    \n",
    "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
    "    with open(f'./data/{pt}/train_data_{MAX_LEN}_sep.pickle', 'wb') as f:\n",
    "        pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1644155676077,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "qW1f2977ruEZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "27998it [00:07, 3663.00it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1644155676077,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "C_s9yG3lr6ha"
   },
   "outputs": [],
   "source": [
    "with open(f'./data/{args.pt}/train_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1644155676078,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "AcMKVufFsdxa",
    "outputId": "f560a737-9f57-47b4-ebc6-2eb1bbc659d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[     0, 105051,  28913, ...,      1,      1,      1],\n",
       "        [     0,  93278,    697, ...,      1,      1,      1],\n",
       "        [     0,  59070,  11270, ...,      1,      1,      1],\n",
       "        ...,\n",
       "        [     0, 240706,   7091, ...,      1,      1,      1],\n",
       "        [     0,      6,  98960, ...,      1,      1,      1],\n",
       "        [     0,      6,  98960, ...,      1,      1,      1]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " 'targets': array([1, 1, 0, ..., 0, 2, 0], dtype=int32)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1644155676594,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "q77laOsvseoo"
   },
   "outputs": [],
   "source": [
    "def preprocessing_test(data):\n",
    "    \n",
    "    pt = args.pt#'monologg/kobert'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pt)\n",
    "    \n",
    "    MAX_LEN = args.max_len\n",
    "    test=data[['premise', 'hypothesis']]\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(test.iterrows()):\n",
    "#         line = preprocessing(row.premise)\n",
    "#         lines.append(line)\n",
    "#     test['premise_pre'] = lines\n",
    "\n",
    "#     lines = []\n",
    "#     for _, row in tqdm.tqdm(test.iterrows()):\n",
    "#         line = preprocessing(row.hypothesis)\n",
    "#         lines.append(line)\n",
    "#     test['hypothesis_pre'] = lines\n",
    "    \n",
    "    input_ids =[]\n",
    "    attention_masks =[]\n",
    "    token_type_ids =[]\n",
    "\n",
    "    for test_premise, test_hypothesis in tqdm.tqdm(zip(test['premise'], test['hypothesis'])):\n",
    "        try:\n",
    "            test_sent = test_premise + \" \" + tokenizer.sep_token + \" \" + test_hypothesis\n",
    "            token= tokenizer(test_sent, max_length=args.max_len, \n",
    "                                                pad_to_max_length=True)\n",
    "\n",
    "            input_ids.append(token['input_ids'])\n",
    "            attention_masks.append(token['attention_mask'])\n",
    "#             token_type_ids.append(token['token_type_ids'])\n",
    "            #########################################\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "\n",
    "    test_input_ids=np.array(input_ids, dtype=int)\n",
    "    test_attention_masks=np.array(attention_masks, dtype=int)\n",
    "#     test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "    ###########################################################\n",
    "    test_inputs=(test_input_ids, test_attention_masks)\n",
    "\n",
    "\n",
    "    # save\n",
    "    test_data = {}\n",
    "\n",
    "    test_data['input_ids'] = test_input_ids\n",
    "    test_data['attention_mask'] = test_attention_masks\n",
    "#     test_data['token_type_ids'] = test_token_type_ids\n",
    "    \n",
    "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
    "    with open(f'./data/{pt}/test_data_{MAX_LEN}_sep.pickle', 'wb') as f:\n",
    "        pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1644155676595,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "miraIC6Fw7qh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1666it [00:00, 3977.93it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1644155676597,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "EAkfqSa3w8oS"
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  dataset\n",
    "# ------------------------\n",
    "class NliDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, data, test=False):\n",
    "        \n",
    "        self.data = data\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        ids = torch.tensor(self.data['input_ids'][idx], dtype=torch.long)\n",
    "        mask = torch.tensor(self.data['attention_mask'][idx], dtype=torch.long)\n",
    "#         token_type_ids = torch.tensor(self.data['token_type_ids'][idx], dtype=torch.long)\n",
    "         \n",
    "            \n",
    "        if self.test:\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "#                 'token_type_ids': token_type_ids\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            target = torch.tensor(self.data['targets'][idx],dtype=torch.long)\n",
    "\n",
    "            return {\n",
    "                    'ids': ids,\n",
    "                    'mask': mask,\n",
    "#                     'token_type_ids': token_type_ids,\n",
    "                    'targets': target\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1644155737762,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "z1u2xbHrSuo1"
   },
   "outputs": [],
   "source": [
    "# - util - #\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "def load_data():\n",
    "    train=pd.read_csv('data/new_train_data.csv')\n",
    "    test=pd.read_csv('data/test_data.csv')\n",
    "    \n",
    "    #\n",
    "    train=train[['premise', 'hypothesis', 'label']]\n",
    "    test=test[['premise', 'hypothesis']]\n",
    "    #\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    train['fold'] = -1\n",
    "    for n_fold, (_,v_idx) in enumerate(skf.split(train, train['label'])):\n",
    "        train.loc[v_idx, 'fold']  = n_fold\n",
    "    train['id'] = [x for x in range(len(train))]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1644155738450,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "7eyie7FFxSG8"
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  scheduler\n",
    "# ------------------------\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    target_lst = []\n",
    "    pred_lst = []\n",
    "    logit = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        ids  = data['ids'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "#         tokentype = data['token_type_ids'].to(device)\n",
    "        target = data['targets'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "                    output = output[0]\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)#.squeeze(0)\n",
    "                loss = loss_fn(output, target)\n",
    "            \n",
    "            val_loss += loss\n",
    "            target_lst.extend(target.detach().cpu().numpy())\n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "        val_mean_loss = val_loss / len(valid_loader)\n",
    "        validation_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "        validation_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "        \n",
    "\n",
    "    return val_mean_loss, validation_score, validation_acc, logit\n",
    "\n",
    "def do_predict(net, valid_loader):\n",
    "    \n",
    "    val_loss = 0\n",
    "    pred_lst = []\n",
    "    logit=[]\n",
    "    net.eval()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        ids  = data['ids'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "#         tokentype = data['token_type_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(ids, mask)[0]\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)\n",
    "             \n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "    return pred_lst,logit\n",
    "\n",
    "def run_train(folds=3):\n",
    "    out_dir = args.dir_+ f'/fold{args.fold}/{args.exp_name}/'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # load dataset\n",
    "    train, test = load_data()    \n",
    "    with open(f'./data/{args.pt}/train_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(f'./data/{args.pt}/test_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "        test_data = pickle.load(f)    \n",
    "    \n",
    "    # split fold\n",
    "    for n_fold in range(5):\n",
    "        print(n_fold)\n",
    "        if n_fold != folds:\n",
    "            print(f'{n_fold} fold pass'+'\\n')\n",
    "            continue\n",
    "            \n",
    "        if args.debug:\n",
    "            train = train.sample(1000).copy()\n",
    "            \n",
    "        print(n_fold)\n",
    "        \n",
    "        trn_idx = train[train['fold']!=n_fold]['id'].values\n",
    "        val_idx = train[train['fold']==n_fold]['id'].values\n",
    "    \n",
    "\n",
    "        train_dict = {'input_ids' : train_data['input_ids'][trn_idx] , 'attention_mask' : train_data['attention_mask'][trn_idx] , \n",
    "                      'targets' : train_data['targets'][trn_idx]}\n",
    "        val_dict = {'input_ids' : train_data['input_ids'][val_idx] , 'attention_mask' : train_data['attention_mask'][val_idx] , \n",
    "                      'targets' : train_data['targets'][val_idx]}\n",
    "\n",
    "        ## dataset ------------------------------------\n",
    "        train_dataset = NliDataSet(data = train_dict)\n",
    "        valid_dataset = NliDataSet(data = val_dict)\n",
    "        trainloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
    "                                 num_workers=8, shuffle=True, pin_memory=True)\n",
    "        validloader = DataLoader(dataset=valid_dataset, batch_size=args.batch_size, \n",
    "                                 num_workers=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "        if 'xlm-roberta' in args.pt:\n",
    "            net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        \n",
    "        elif 'klue/roberta' in args.pt:\n",
    "            net = RobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        else:\n",
    "            net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "\n",
    "        net.to(device)\n",
    "        if len(args.gpu)>1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "        # ------------------------\n",
    "        # loss\n",
    "        # ------------------------\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        optimizer = optim.Lookahead(optim.RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(trainloader)*args.epochs)\n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        start_timer = timer()\n",
    "        best_score = 0\n",
    "        early_stopping = 0\n",
    "\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            train_loss = 0\n",
    "            valid_loss = 0\n",
    "\n",
    "            target_lst = []\n",
    "            pred_lst = []\n",
    "            lr = get_learning_rate(optimizer)\n",
    "            print(f'-------------------')\n",
    "            print(f'{epoch}epoch start')\n",
    "            print(f'-------------------'+'\\n')\n",
    "            print(f'learning rate : {lr : .6f}')\n",
    "            for t, data in enumerate(tqdm.tqdm(trainloader)):\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                ids  = data['ids'].to(device)\n",
    "                mask  = data['mask'].to(device)\n",
    "#                 tokentype = data['token_type_ids'].to(device)\n",
    "                target = data['targets'].to(device)\n",
    "\n",
    "                # ------------\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                if args.amp:\n",
    "                    with amp.autocast():\n",
    "                        # output\n",
    "                        output = net(ids, mask)\n",
    "                        output = output[0]\n",
    "\n",
    "                        # loss\n",
    "                        loss = loss_fn(output, target)\n",
    "                        train_loss += loss\n",
    "\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else:\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "                    train_loss += loss\n",
    "\n",
    "                    # update\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # for calculate f1 score\n",
    "                target_lst.extend(target.detach().cpu().numpy())\n",
    "                pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "            train_loss = train_loss / len(trainloader)\n",
    "            train_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "            train_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "\n",
    "            # validation\n",
    "            valid_loss, valid_score, valid_acc, _ = do_valid(net, validloader)\n",
    "\n",
    "\n",
    "            if valid_acc > best_score:\n",
    "                best_score = valid_acc\n",
    "                best_epoch = epoch\n",
    "                best_loss = valid_loss\n",
    "\n",
    "                torch.save(net.state_dict(), out_dir + f'/{folds}f.pth')\n",
    "                print('best model saved'+'\\n')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "\n",
    "            print(f'train loss : {train_loss:.4f}, train f1 score : {train_score : .4f}, train acc : {train_acc : .4f}'+'\\n')\n",
    "            print(f'valid loss : {valid_loss:.4f}, valid f1 score : {valid_score : .4f}, valid acc : {valid_acc : .4f}'+'\\n')\n",
    "\n",
    "\n",
    "        print(f'best valid loss : {best_loss : .4f}'+'\\n')\n",
    "        print(f'best epoch : {best_epoch }'+'\\n')\n",
    "        print(f'best accuracy : {best_score : .4f}'+'\\n')\n",
    "        \n",
    "def run_predict(model_path):\n",
    "    ## dataset ------------------------------------\n",
    "    # load\n",
    "    with open(f'./data/{args.pt}/test_data_{args.max_len}_sep.pickle', 'rb') as f:\n",
    "        test_dict = pickle.load(f)\n",
    "        \n",
    "    print('test load')\n",
    "    test_dataset = NliDataSet(data = test_dict, test=True)\n",
    "    testloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, \n",
    "                             num_workers=8, shuffle=False, pin_memory=True)\n",
    "    print('set testloader')\n",
    "    ## net ----------------------------------------\n",
    "    scaler = amp.GradScaler()\n",
    "    if 'xlm-roberta' in args.pt:\n",
    "        net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        \n",
    "    elif 'klue/roberta' in args.pt:\n",
    "        net = RobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "    else:\n",
    "        net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 3) \n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    if len(args.gpu)>1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    f = torch.load(model_path)\n",
    "    net.load_state_dict(f, strict=True)  # True\n",
    "    print('load saved models')\n",
    "    # ------------------------\n",
    "    # validation\n",
    "    preds, logit = do_predict(net, testloader) #outputs\n",
    "           \n",
    "    print('complete predict')\n",
    "    \n",
    "    return preds, np.array(logit)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709,
     "referenced_widgets": [
      "7f2c0d3d34264e6ca05b3643e8de63e8",
      "bc5bc0135f5547cc921ea4dab74d7ae5",
      "001eba30d4be45b190c7ff47c41b2b4e",
      "87b45d1120754a8d90682eb6ee5a2b43",
      "d135467aa18647b284fcda02303293fd",
      "9628f91e1a0a4ed1bf72f4c6d633eb32",
      "82be25f2bb384d73b5f5d97d376b8c66",
      "4d592663a0404021bedaed03624de2f7",
      "31140f3b19744da5a893143ba86f7692",
      "78a9c5fd0a4f4d4a8c66770464d69288",
      "6d4a77754f7d4afe99bb02c82a5060ee"
     ]
    },
    "executionInfo": {
     "elapsed": 112961,
     "status": "error",
     "timestamp": 1644155851408,
     "user": {
      "displayName": "김현민",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14696096218232685117"
     },
     "user_tz": -540
    },
    "id": "U2BclMLgTG10",
    "outputId": "063f91bc-896a-4039-e639-2d362965e98d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 fold pass\n",
      "\n",
      "1\n",
      "1 fold pass\n",
      "\n",
      "2\n",
      "2 fold pass\n",
      "\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.5931, train f1 score :  0.7253, train acc :  0.7259\n",
      "\n",
      "valid loss : 0.3002, valid f1 score :  0.8914, valid acc :  0.8918\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:49<00:00,  2.41it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2479, train f1 score :  0.9122, train acc :  0.9128\n",
      "\n",
      "valid loss : 0.2979, valid f1 score :  0.9024, valid acc :  0.9028\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1417, train f1 score :  0.9524, train acc :  0.9527\n",
      "\n",
      "valid loss : 0.3167, valid f1 score :  0.9055, valid acc :  0.9059\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0855, train f1 score :  0.9734, train acc :  0.9735\n",
      "\n",
      "valid loss : 0.3680, valid f1 score :  0.9034, valid acc :  0.9039\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0581, train f1 score :  0.9808, train acc :  0.9808\n",
      "\n",
      "valid loss : 0.4003, valid f1 score :  0.9060, valid acc :  0.9068\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0416, train f1 score :  0.9873, train acc :  0.9874\n",
      "\n",
      "valid loss : 0.4266, valid f1 score :  0.9073, valid acc :  0.9080\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0318, train f1 score :  0.9897, train acc :  0.9898\n",
      "\n",
      "valid loss : 0.4316, valid f1 score :  0.9080, valid acc :  0.9082\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:54<00:00,  2.37it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0262, train f1 score :  0.9916, train acc :  0.9917\n",
      "\n",
      "valid loss : 0.4925, valid f1 score :  0.9024, valid acc :  0.9028\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0236, train f1 score :  0.9925, train acc :  0.9925\n",
      "\n",
      "valid loss : 0.4912, valid f1 score :  0.9033, valid acc :  0.9039\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0194, train f1 score :  0.9944, train acc :  0.9944\n",
      "\n",
      "valid loss : 0.5615, valid f1 score :  0.9014, valid acc :  0.9025\n",
      "\n",
      "-------------------\n",
      "11epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best valid loss :  0.4316\n",
      "\n",
      "best epoch : 7\n",
      "\n",
      "best accuracy :  0.9082\n",
      "\n",
      "4\n",
      "4 fold pass\n",
      "\n",
      "0\n",
      "0 fold pass\n",
      "\n",
      "1\n",
      "1 fold pass\n",
      "\n",
      "2\n",
      "2 fold pass\n",
      "\n",
      "3\n",
      "3 fold pass\n",
      "\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.6353, train f1 score :  0.6935, train acc :  0.6948\n",
      "\n",
      "valid loss : 0.2984, valid f1 score :  0.8921, valid acc :  0.8928\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.44it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2558, train f1 score :  0.9090, train acc :  0.9095\n",
      "\n",
      "valid loss : 0.2807, valid f1 score :  0.9032, valid acc :  0.9034\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1430, train f1 score :  0.9519, train acc :  0.9522\n",
      "\n",
      "valid loss : 0.2982, valid f1 score :  0.9049, valid acc :  0.9052\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.43it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0800, train f1 score :  0.9754, train acc :  0.9755\n",
      "\n",
      "valid loss : 0.3580, valid f1 score :  0.9118, valid acc :  0.9123\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.44it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0569, train f1 score :  0.9807, train acc :  0.9808\n",
      "\n",
      "valid loss : 0.3816, valid f1 score :  0.9097, valid acc :  0.9102\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.44it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0377, train f1 score :  0.9878, train acc :  0.9879\n",
      "\n",
      "valid loss : 0.4249, valid f1 score :  0.9059, valid acc :  0.9064\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.44it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0331, train f1 score :  0.9894, train acc :  0.9895\n",
      "\n",
      "valid loss : 0.4023, valid f1 score :  0.9104, valid acc :  0.9111\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.42it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0263, train f1 score :  0.9913, train acc :  0.9913\n",
      "\n",
      "valid loss : 0.4335, valid f1 score :  0.9114, valid acc :  0.9121\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:47<00:00,  2.44it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0212, train f1 score :  0.9933, train acc :  0.9933\n",
      "\n",
      "valid loss : 0.4157, valid f1 score :  0.9126, valid acc :  0.9128\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [04:48<00:00,  2.42it/s]\n",
      "100%|██████████| 175/175 [00:25<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best valid loss :  0.4157\n",
      "\n",
      "best epoch : 9\n",
      "\n",
      "best accuracy :  0.9128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5fold 전용\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for pt, max_len in zip(['xlm-roberta-large', 'klue/roberta-large'],[194, 194]):\n",
    "        \n",
    "        args.max_len = max_len\n",
    "        args.pt = pt\n",
    "        args.exp_name = str(args.pt) + '_' + str(args.max_len)\n",
    "    \n",
    "        for i in [0,1,2,3,4]: # 5fold\n",
    "            run_train(folds=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    final_logit=0\n",
    "    \n",
    "    args.pt = 'klue/roberta-large'\n",
    "    _, logit1 = run_predict(\"./saved_models/fold5/klue/roberta-large_194/0f.pth\")\n",
    "    _, logit2 = run_predict(\"./saved_models/fold5/klue/roberta-large_194/1f.pth\")\n",
    "    _, logit3 = run_predict(\"./saved_models/fold5/klue/roberta-large_194/2f.pth\")\n",
    "    _, logit4 = run_predict(\"./saved_models/fold5/klue/roberta-large_194/3f.pth\")\n",
    "    _, logit5 = run_predict(\"./saved_models/fold5/klue/roberta-large_194/4f.pth\")\n",
    "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
    "    \n",
    "    args.pt = 'xlm-roberta-large'\n",
    "    _, logit1 = run_predict(\"./saved_models/fold5/xlm-roberta-large_194/0f.pth\")\n",
    "    _, logit2 = run_predict(\"./saved_models/fold5/xlm-roberta-large_194/1f.pth\")\n",
    "    _, logit3 = run_predict(\"./saved_models/fold5/xlm-roberta-large_194/2f.pth\")\n",
    "    _, logit4 = run_predict(\"./saved_models/fold5/xlm-roberta-large_194/3f.pth\")\n",
    "    _, logit5 = run_predict(\"./saved_models/fold5/xlm-roberta-large_194/4f.pth\")\n",
    "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
    "    \n",
    "    \n",
    "    return final_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_logit = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.71816406, 11.7234375 , -6.41796875],\n",
       "       [-1.07399902, -7.2671875 ,  8.77460938],\n",
       "       [ 6.01160278, -5.44648437, -0.73701172],\n",
       "       ...,\n",
       "       [-3.97392578, -6.8140625 , 11.19257813],\n",
       "       [-4.28496094, -6.88242188, 11.36875   ],\n",
       "       [-2.63457031, -1.33371582,  4.165625  ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./robert_npy', final_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "# out = [list(label_dict.keys())[_] for _ in final_logit.argmax(1)]\n",
    "\n",
    "# sub['label'] = out\n",
    "# print(sub)\n",
    "# # preds\n",
    "# sub.to_csv(f'./submission/final_submission_roberta-large_200.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMH1FJMRvE67Iv93Ol6JIih",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "nli_ver0.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001eba30d4be45b190c7ff47c41b2b4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82be25f2bb384d73b5f5d97d376b8c66",
      "placeholder": "​",
      "style": "IPY_MODEL_9628f91e1a0a4ed1bf72f4c6d633eb32",
      "value": "Downloading: 100%"
     }
    },
    "31140f3b19744da5a893143ba86f7692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d592663a0404021bedaed03624de2f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6d4a77754f7d4afe99bb02c82a5060ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78a9c5fd0a4f4d4a8c66770464d69288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f2c0d3d34264e6ca05b3643e8de63e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_001eba30d4be45b190c7ff47c41b2b4e",
       "IPY_MODEL_87b45d1120754a8d90682eb6ee5a2b43",
       "IPY_MODEL_d135467aa18647b284fcda02303293fd"
      ],
      "layout": "IPY_MODEL_bc5bc0135f5547cc921ea4dab74d7ae5"
     }
    },
    "82be25f2bb384d73b5f5d97d376b8c66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87b45d1120754a8d90682eb6ee5a2b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31140f3b19744da5a893143ba86f7692",
      "max": 442560877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4d592663a0404021bedaed03624de2f7",
      "value": 442560877
     }
    },
    "9628f91e1a0a4ed1bf72f4c6d633eb32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc5bc0135f5547cc921ea4dab74d7ae5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d135467aa18647b284fcda02303293fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d4a77754f7d4afe99bb02c82a5060ee",
      "placeholder": "​",
      "style": "IPY_MODEL_78a9c5fd0a4f4d4a8c66770464d69288",
      "value": " 422M/422M [00:07&lt;00:00, 61.3MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
