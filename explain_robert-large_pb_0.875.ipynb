{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ LIBRARY -------#\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
    "#\n",
    "\n",
    "import math\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch_optimizer as optim\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "#import time\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# transformer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# class args\n",
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    amp = True\n",
    "    gpu = '0'\n",
    "    \n",
    "    epochs=10\n",
    "    batch_size=16\n",
    "    weight_decay=1e-6\n",
    "    n_fold=5\n",
    "    fold=5 # [0, 1, 2, 3, 4] # 원래는 3\n",
    "    \n",
    "    exp_name = 'experiment_name_folder'\n",
    "    dir_ = f'./saved_models/'\n",
    "    pt = 'klue/roberta-large'\n",
    "    max_len = 200\n",
    "    \n",
    "    start_lr = 2e-5#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=2021\n",
    "    scheduler = None#'get_linear_schedule_with_warmup'\n",
    "\n",
    "\n",
    "data_dir = './'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
    "\n",
    "set_seeds(seed=args.seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_to_max_length(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n",
    "    List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    pad to maximum length of this batch\n",
    "    Args:\n",
    "        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n",
    "        max_len: specify max length\n",
    "        fill_values: specify filled values of each field\n",
    "    Returns:\n",
    "        output: list of field batched data, which shape is [batch, max_length]\n",
    "    \"\"\"\n",
    "    # [batch, num_fields]\n",
    "    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n",
    "    batch_size, num_fields = lengths.shape\n",
    "    fill_values = fill_values or [0.0] * num_fields\n",
    "    # [num_fields]\n",
    "    max_lengths = lengths.max(axis=0)\n",
    "    if max_len:\n",
    "        assert max_lengths.max() <= max_len\n",
    "        max_lengths = np.ones_like(max_lengths) * max_len\n",
    "\n",
    "    output = [torch.full([batch_size, max_lengths[field_idx]],\n",
    "                         fill_value=fill_values[field_idx],\n",
    "                         dtype=batch[0][field_idx].dtype)\n",
    "              for field_idx in range(num_fields)]\n",
    "    for sample_idx in range(batch_size):\n",
    "        for field_idx in range(num_fields):\n",
    "            # seq_length\n",
    "            data = batch[sample_idx][field_idx]\n",
    "            output[field_idx][sample_idx][: data.shape[0]] = data\n",
    "    # generate span_index and span_mask\n",
    "    max_sentence_length = max_lengths[0]\n",
    "    start_indexs = []\n",
    "    end_indexs = []\n",
    "    for i in range(1, max_sentence_length - 1):\n",
    "        for j in range(i, max_sentence_length - 1):\n",
    "            # # span大小为10\n",
    "            # if j - i > 10:\n",
    "            #     continue\n",
    "            start_indexs.append(i)\n",
    "            end_indexs.append(j)\n",
    "    # generate span mask\n",
    "    span_masks = []\n",
    "    for input_ids, label, length in batch:\n",
    "        span_mask = []\n",
    "        middle_index = input_ids.tolist().index(2)\n",
    "        for start_index, end_index in zip(start_indexs, end_indexs):\n",
    "            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n",
    "                start_index > middle_index or end_index < middle_index):\n",
    "                span_mask.append(0)\n",
    "            else:\n",
    "                span_mask.append(1e6)\n",
    "        span_masks.append(span_mask)\n",
    "    # add to output\n",
    "    output.append(torch.LongTensor(start_indexs))\n",
    "    output.append(torch.LongTensor(end_indexs))\n",
    "    output.append(torch.LongTensor(span_masks))\n",
    "    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_to_max_length_test(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n",
    "    List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    pad to maximum length of this batch\n",
    "    Args:\n",
    "        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n",
    "        max_len: specify max length\n",
    "        fill_values: specify filled values of each field\n",
    "    Returns:\n",
    "        output: list of field batched data, which shape is [batch, max_length]\n",
    "    \"\"\"\n",
    "    # [batch, num_fields]\n",
    "    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n",
    "    batch_size, num_fields = lengths.shape\n",
    "    fill_values = fill_values or [0.0] * num_fields\n",
    "    # [num_fields]\n",
    "    max_lengths = lengths.max(axis=0)\n",
    "    if max_len:\n",
    "        assert max_lengths.max() <= max_len\n",
    "        max_lengths = np.ones_like(max_lengths) * max_len\n",
    "\n",
    "    output = [torch.full([batch_size, max_lengths[field_idx]],\n",
    "                         fill_value=fill_values[field_idx],\n",
    "                         dtype=batch[0][field_idx].dtype)\n",
    "              for field_idx in range(num_fields)]\n",
    "    for sample_idx in range(batch_size):\n",
    "        for field_idx in range(num_fields):\n",
    "            # seq_length\n",
    "            data = batch[sample_idx][field_idx]\n",
    "            output[field_idx][sample_idx][: data.shape[0]] = data\n",
    "    # generate span_index and span_mask\n",
    "    max_sentence_length = max_lengths[0]\n",
    "    start_indexs = []\n",
    "    end_indexs = []\n",
    "    for i in range(1, max_sentence_length - 1):\n",
    "        for j in range(i, max_sentence_length - 1):\n",
    "            # # span大小为10\n",
    "            # if j - i > 10:\n",
    "            #     continue\n",
    "            start_indexs.append(i)\n",
    "            end_indexs.append(j)\n",
    "    # generate span mask\n",
    "    span_masks = []\n",
    "    for input_ids, length in batch:\n",
    "        span_mask = []\n",
    "        middle_index = input_ids.tolist().index(2)\n",
    "        for start_index, end_index in zip(start_indexs, end_indexs):\n",
    "            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n",
    "                start_index > middle_index or end_index < middle_index):\n",
    "                span_mask.append(0)\n",
    "            else:\n",
    "                span_mask.append(1e6)\n",
    "        span_masks.append(span_mask)\n",
    "    # add to output\n",
    "    output.append(torch.LongTensor(start_indexs))\n",
    "    output.append(torch.LongTensor(end_indexs))\n",
    "    output.append(torch.LongTensor(span_masks))\n",
    "    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\")\n",
    "test = pd.read_csv(\"data/test_data.csv\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction' 'entailment' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(train[\"label\"]))\n",
    "\n",
    "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(train.label):\n",
    "    train.label[i] = label_dict[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(args.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, bert_path, max_length: int = 512):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.result = []\n",
    "        for train_premise, train_hypothesis, train_label in tqdm.tqdm(zip(data['premise'], data['hypothesis'], data['label'])):\n",
    "                    self.result.append((train_premise, train_hypothesis, train_label))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_1, sentence_2, label = self.result[idx]\n",
    "        # remove .\n",
    "        if sentence_1.endswith(\".\"):\n",
    "            sentence_1 = sentence_1[:-1]\n",
    "        if sentence_2.endswith(\".\"):\n",
    "            sentence_2 = sentence_2[:-1]\n",
    "        sentence_1_input_ids = self.tokenizer.encode(sentence_1, add_special_tokens=False)\n",
    "        sentence_2_input_ids = self.tokenizer.encode(sentence_2, add_special_tokens=False)\n",
    "        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n",
    "        if len(input_ids) > self.max_length - 2:\n",
    "            input_ids = input_ids[:self.max_length - 2]\n",
    "        # convert list to tensor\n",
    "        length = torch.LongTensor([len(input_ids) + 2])\n",
    "        input_ids = torch.LongTensor([0] + input_ids + [2])\n",
    "        label = torch.LongTensor([label])\n",
    "        \n",
    "        return input_ids, label, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self, data, bert_path, max_length: int = 512):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.result = []\n",
    "        for test_premise, test_hypothesis in tqdm.tqdm(zip(data['premise'], data['hypothesis'])):\n",
    "                    self.result.append((test_premise, test_hypothesis))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_1, sentence_2 = self.result[idx]\n",
    "        # remove .\n",
    "        if sentence_1.endswith(\".\"):\n",
    "            sentence_1 = sentence_1[:-1]\n",
    "        if sentence_2.endswith(\".\"):\n",
    "            sentence_2 = sentence_2[:-1]\n",
    "        sentence_1_input_ids = self.tokenizer.encode(sentence_1, add_special_tokens=False)\n",
    "        sentence_2_input_ids = self.tokenizer.encode(sentence_2, add_special_tokens=False)\n",
    "        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n",
    "        if len(input_ids) > self.max_length - 2:\n",
    "            input_ids = input_ids[:self.max_length - 2]\n",
    "        # convert list to tensor\n",
    "        length = torch.LongTensor([len(input_ids) + 2])\n",
    "        input_ids = torch.LongTensor([0] + input_ids + [2])\n",
    "        \n",
    "        return input_ids, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test():\n",
    "    \n",
    "    dataset = SNLIDataset(data=train[:1], bert_path=args.pt)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=10,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0])\n",
    "    )\n",
    "    for input_ids, label, length, start_index, end_index, span_mask in dataloader:\n",
    "        print(input_ids.shape, input_ids)\n",
    "        print(start_index.shape, start_index)\n",
    "        print(end_index.shape, end_index)\n",
    "        print(span_mask.shape, span_mask)\n",
    "        print(label.view(-1).shape, label)\n",
    "        print()\n",
    "        \n",
    "    for t, data in enumerate(tqdm.tqdm(dataloader)):\n",
    "        print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 8097.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51]) tensor([[    0, 14441,  2073, 12382, 13169,  2200,  3797, 21505,  9005,  2259,\n",
      "          3997,  2031,  2079,  3661, 31221,  5845,  2200,  2112,    16,  5950,\n",
      "         15351, 17788,  7285,   748,  2088, 22048,  2470,  1132, 21893, 15351,\n",
      "          6481, 27135,  5417,  4084,  1972,  2145, 17524,  2138, 15526,  2259,\n",
      "           575, 28674,     2, 14441,  2079,  3883,  2031,  2079,  5845, 28674,\n",
      "             2]])\n",
      "torch.Size([1225]) tensor([ 1,  1,  1,  ..., 48, 48, 49])\n",
      "torch.Size([1225]) tensor([ 1,  2,  3,  ..., 48, 49, 49])\n",
      "torch.Size([1, 1225]) tensor([[0, 0, 0,  ..., 0, 0, 0]])\n",
      "torch.Size([1]) tensor([[1]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 262.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 14441,  2073, 12382, 13169,  2200,  3797, 21505,  9005,  2259,\n",
      "          3997,  2031,  2079,  3661, 31221,  5845,  2200,  2112,    16,  5950,\n",
      "         15351, 17788,  7285,   748,  2088, 22048,  2470,  1132, 21893, 15351,\n",
      "          6481, 27135,  5417,  4084,  1972,  2145, 17524,  2138, 15526,  2259,\n",
      "           575, 28674,     2, 14441,  2079,  3883,  2031,  2079,  5845, 28674,\n",
      "             2]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableModel(nn.Module):\n",
    "    def __init__(self, bert_dir):\n",
    "        super().__init__()\n",
    "        self.bert_config = AutoConfig.from_pretrained(bert_dir, output_hidden_states=False)\n",
    "        self.intermediate = AutoModel.from_pretrained(bert_dir, return_dict=False)\n",
    "        self.span_info_collect = SICModel(self.bert_config.hidden_size)\n",
    "        self.interpretation = InterpretationModel(self.bert_config.hidden_size)\n",
    "        self.output = nn.Linear(self.bert_config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n",
    "        # generate mask\n",
    "        attention_mask = (input_ids != 1).long()\n",
    "        # intermediate layer\n",
    "        hidden_states, first_token = self.intermediate(input_ids, attention_mask=attention_mask)  # output.shape = (bs, length, hidden_size)\n",
    "        # span info collecting layer(SIC)\n",
    "        h_ij = self.span_info_collect(hidden_states, start_indexs, end_indexs)\n",
    "        # interpretation layer\n",
    "        H, a_ij = self.interpretation(h_ij, span_masks)\n",
    "        # output layer\n",
    "        out = self.output(H)\n",
    "        return out, a_ij\n",
    "\n",
    "\n",
    "class SICModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, start_indexs, end_indexs):\n",
    "        W1_h = self.W_1(hidden_states)  # (bs, length, hidden_size)\n",
    "        W2_h = self.W_2(hidden_states)\n",
    "        W3_h = self.W_3(hidden_states)\n",
    "        W4_h = self.W_4(hidden_states)\n",
    "\n",
    "        W1_hi_emb = torch.index_select(W1_h, 1, start_indexs)  # (bs, span_num, hidden_size)\n",
    "        W2_hj_emb = torch.index_select(W2_h, 1, end_indexs)\n",
    "        W3_hi_start_emb = torch.index_select(W3_h, 1, start_indexs)\n",
    "        W3_hi_end_emb = torch.index_select(W3_h, 1, end_indexs)\n",
    "        W4_hj_start_emb = torch.index_select(W4_h, 1, start_indexs)\n",
    "        W4_hj_end_emb = torch.index_select(W4_h, 1, end_indexs)\n",
    "\n",
    "        # [w1*hi, w2*hj, w3(hi-hj), w4(hi⊗hj)]\n",
    "        span = W1_hi_emb + W2_hj_emb + (W3_hi_start_emb - W3_hi_end_emb) + torch.mul(W4_hj_start_emb, W4_hj_end_emb)\n",
    "        h_ij = torch.tanh(span)\n",
    "        return h_ij\n",
    "\n",
    "\n",
    "class InterpretationModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.h_t = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, h_ij, span_masks):\n",
    "        o_ij = self.h_t(h_ij).squeeze(-1)  # (ba, span_num)\n",
    "        # mask illegal span\n",
    "        o_ij = o_ij - span_masks\n",
    "        # normalize all a_ij, a_ij sum = 1\n",
    "        a_ij = nn.functional.softmax(o_ij, dim=1)\n",
    "        # weight average span representation to get H\n",
    "        H = (a_ij.unsqueeze(-1) * h_ij).sum(dim=1)  # (bs, hidden_size)\n",
    "        return H, a_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - util - #\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "def load_data():\n",
    "    train=pd.read_csv('data/train_data.csv')\n",
    "    test=pd.read_csv('data/test_data.csv')\n",
    "    \n",
    "    #\n",
    "    train=train[['premise', 'hypothesis', 'label']]\n",
    "    test=test[['premise', 'hypothesis']]\n",
    "    \n",
    "    #\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    train['fold'] = -1\n",
    "    for n_fold, (_,v_idx) in enumerate(skf.split(train, train['label'])):\n",
    "        train.loc[v_idx, 'fold']  = n_fold\n",
    "    train['id'] = [x for x in range(len(train))]\n",
    "    \n",
    "    for i, text in enumerate(train.label):\n",
    "        train.label[i] = label_dict[text]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  scheduler\n",
    "# ------------------------\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    target_lst = []\n",
    "    pred_lst = []\n",
    "    logit = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "\n",
    "        # (input_ids, labels, length, start_indexs, end_indexs, span_masks)\n",
    "        input_ids  = data[0].to(device)\n",
    "        start_index  = data[3].to(device)\n",
    "        end_index = data[4].to(device)\n",
    "        span_mask = data[5].to(device)\n",
    "        target = data[1].to(device).view(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output, a_ij = net(input_ids, start_index, end_index, span_mask)\n",
    "#                     output = output[0]\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)#.squeeze(0)\n",
    "                loss = loss_fn(output, target)\n",
    "            \n",
    "            val_loss += loss\n",
    "            target_lst.extend(target.detach().cpu().numpy())\n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "        val_mean_loss = val_loss / len(valid_loader)\n",
    "        validation_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "        validation_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "        \n",
    "\n",
    "    return val_mean_loss, validation_score, validation_acc, logit\n",
    "\n",
    "def do_predict(net, valid_loader):\n",
    "    \n",
    "    val_loss = 0\n",
    "    pred_lst = []\n",
    "    logit=[]\n",
    "    net.eval()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        \n",
    "        # (input_ids, length, start_indexs, end_indexs, span_masks)\n",
    "        input_ids  = data[0].to(device)\n",
    "        start_index  = data[2].to(device)\n",
    "        end_index = data[3].to(device)\n",
    "        span_mask = data[4].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(input_ids, start_index, end_index, span_mask)[0]\n",
    "\n",
    "            else:\n",
    "                output = net(input_ids, start_index, end_index, span_mask)\n",
    "             \n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "    return pred_lst,logit\n",
    "\n",
    "def run_train(folds=3):\n",
    "    out_dir = args.dir_+ f'/fold{args.fold}/{args.exp_name}/'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # load dataset\n",
    "    train, test = load_data()    \n",
    " \n",
    "    \n",
    "    # split fold\n",
    "    for n_fold in range(5):\n",
    "        print(n_fold)\n",
    "        if n_fold != folds:\n",
    "            print(f'{n_fold} fold pass'+'\\n')\n",
    "            continue\n",
    "            \n",
    "        if args.debug:\n",
    "            train = train.sample(1000).copy()\n",
    "            \n",
    "        print(n_fold)\n",
    "        \n",
    "        trn_idx = train[train['fold']!=n_fold]['id'].values\n",
    "        val_idx = train[train['fold']==n_fold]['id'].values\n",
    "        print(train.iloc[trn_idx])\n",
    "    \n",
    "\n",
    "        ## dataset ------------------------------------\n",
    "        train_dataset = SNLIDataset(data = train.iloc[trn_idx], bert_path=args.pt)\n",
    "        valid_dataset = SNLIDataset(data = train.iloc[val_idx], bert_path=args.pt)\n",
    "        trainloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
    "                                 num_workers=8, shuffle=True, pin_memory=True, collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]))\n",
    "        validloader = DataLoader(dataset=valid_dataset, batch_size=args.batch_size, \n",
    "                                 num_workers=8, shuffle=False, pin_memory=True, collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]))\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "        net = ExplainableModel(args.pt)\n",
    "\n",
    "        net.to(device)\n",
    "        if len(args.gpu)>1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "        # ------------------------\n",
    "        # loss\n",
    "        # ------------------------\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in net.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in net.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          betas=(0.9, 0.98),  # according to RoBERTa paper\n",
    "                          lr=args.start_lr,\n",
    "                          eps=1e-9)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(trainloader)*args.epochs)\n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        start_timer = timer()\n",
    "        best_score = 0\n",
    "\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            train_loss = 0\n",
    "            valid_loss = 0\n",
    "\n",
    "            target_lst = []\n",
    "            pred_lst = []\n",
    "#             lr = get_learning_rate(optimizer)\n",
    "            print(f'-------------------')\n",
    "            print(f'{epoch}epoch start')\n",
    "            print(f'-------------------'+'\\n')\n",
    "#             print(f'learning rate : {lr : .6f}')\n",
    "            for t, data in enumerate(tqdm.tqdm(trainloader)):\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                input_ids  = data[0].to(device)\n",
    "                start_index  = data[3].to(device)\n",
    "                end_index = data[4].to(device)\n",
    "                span_mask = data[5].to(device)\n",
    "                target = data[1].to(device).view(-1)\n",
    "\n",
    "                # ------------\n",
    "#                 net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                if args.amp:\n",
    "                    with amp.autocast():\n",
    "                        # output\n",
    "                        output, a_ij = net(input_ids, start_index, end_index, span_mask)\n",
    "#                         output = output[0]\n",
    "\n",
    "                        # loss\n",
    "                        loss = loss_fn(output, target)\n",
    "                        train_loss += loss\n",
    "\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else:\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "                    train_loss += loss\n",
    "\n",
    "                    # update\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # for calculate f1 score\n",
    "                target_lst.extend(target.detach().cpu().numpy())\n",
    "                pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "            train_loss = train_loss / len(trainloader)\n",
    "            train_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "            train_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "\n",
    "            # validation\n",
    "            valid_loss, valid_score, valid_acc, _ = do_valid(net, validloader)\n",
    "\n",
    "\n",
    "            if valid_acc > best_score:\n",
    "                best_score = valid_acc\n",
    "                best_epoch = epoch\n",
    "                best_loss = valid_loss\n",
    "\n",
    "                torch.save(net.state_dict(), out_dir + f'/{folds}f_explain.pth')\n",
    "                print('best model saved'+'\\n')\n",
    "\n",
    "\n",
    "            print(f'train loss : {train_loss:.4f}, train f1 score : {train_score : .4f}, train acc : {train_acc : .4f}'+'\\n')\n",
    "            print(f'valid loss : {valid_loss:.4f}, valid f1 score : {valid_score : .4f}, valid acc : {valid_acc : .4f}'+'\\n')\n",
    "\n",
    "\n",
    "        print(f'best valid loss : {best_loss : .4f}'+'\\n')\n",
    "        print(f'best epoch : {best_epoch }'+'\\n')\n",
    "        print(f'best accuracy : {best_score : .4f}'+'\\n')\n",
    "        \n",
    "def run_predict(model_path):\n",
    "    ## dataset ------------------------------------\n",
    "    # load\n",
    "        \n",
    "    train, test = load_data()\n",
    "    print('test load')\n",
    "\n",
    "    test_dataset = SNLIDataset_test(data = test, bert_path=args.pt)\n",
    "    testloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, \n",
    "                             num_workers=8, shuffle=False, pin_memory=True, collate_fn=partial(collate_to_max_length_test, fill_values=[1, 0, 0]))\n",
    "    print('set testloader')\n",
    "    ## net ----------------------------------------\n",
    "    scaler = amp.GradScaler()\n",
    "    net = ExplainableModel(args.pt)\n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    if len(args.gpu)>1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    f = torch.load(model_path)\n",
    "    net.load_state_dict(f, strict=True)  # True\n",
    "    print('load saved models')\n",
    "    # ------------------------\n",
    "    # validation\n",
    "    preds, logit = do_predict(net, testloader) #outputs\n",
    "           \n",
    "    print('complete predict')\n",
    "    \n",
    "    return preds, np.array(logit)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2e91797aca71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 5fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8ff3f7c45949>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(folds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ac51c8de71b7>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3603\u001b[0m             \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3605\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"referant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3607\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[0;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[1;32m   3683\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSettingWithCopyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3684\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"warn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3685\u001b[0;31m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSettingWithCopyWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"5fold 전용\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args.exp_name = str(args.pt) + '_' + str(args.max_len)\n",
    "    \n",
    "    for i in [0,1,2,3,4]: # 5fold\n",
    "        run_train(folds=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    final_logit=0\n",
    "    \n",
    "    _, logit1 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/0f_explain.pth\")\n",
    "    _, logit2 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/1f_explain.pth\")\n",
    "    _, logit3 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/2f_explain.pth\")\n",
    "    _, logit4 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/3f_explain.pth\")\n",
    "    _, logit5 = run_predict(\"./saved_models/fold5/klue/roberta-large_200/4f_explain.pth\")\n",
    "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
    "    \n",
    "    \n",
    "    return final_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                premise  \\\n",
      "0                             다만 조금 좁아서 케리어를 펼치기 불편합니다.   \n",
      "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요   \n",
      "2                     구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.   \n",
      "3                              몇 번을 다시봐도 볼 때마다 가슴이 저민다.   \n",
      "4          8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.   \n",
      "...                                                 ...   \n",
      "1661  또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...   \n",
      "1662                   결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.   \n",
      "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...   \n",
      "1664                            로마에서 3박4일간 이곳에서 머물렀습니다.   \n",
      "1665                             난 당신이 떠날때 길 하나도 못 건넜는데   \n",
      "\n",
      "                                hypothesis  \n",
      "0                    케리어를 펼치기에 공간이 충분했습니다.  \n",
      "1               시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요  \n",
      "2        무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다.  \n",
      "3                           다시 봤을때는 무덤덤했다.  \n",
      "4     8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다.  \n",
      "...                                    ...  \n",
      "1661          이미지 데이터를 가공하는 것이 가장 난이도가 높다.  \n",
      "1662             결말을 보니 분명히 2편이 나올것이 틀림없다.  \n",
      "1663  사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다.  \n",
      "1664            이곳에서 머무르며 로마의 명소들을 방문했습니다.  \n",
      "1665                 난 당신이 떤나 사실도 모르고 있었는데  \n",
      "\n",
      "[1666 rows x 2 columns]\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 716174.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 19.11it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "                                                premise  \\\n",
      "0                             다만 조금 좁아서 케리어를 펼치기 불편합니다.   \n",
      "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요   \n",
      "2                     구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.   \n",
      "3                              몇 번을 다시봐도 볼 때마다 가슴이 저민다.   \n",
      "4          8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.   \n",
      "...                                                 ...   \n",
      "1661  또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...   \n",
      "1662                   결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.   \n",
      "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...   \n",
      "1664                            로마에서 3박4일간 이곳에서 머물렀습니다.   \n",
      "1665                             난 당신이 떠날때 길 하나도 못 건넜는데   \n",
      "\n",
      "                                hypothesis  \n",
      "0                    케리어를 펼치기에 공간이 충분했습니다.  \n",
      "1               시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요  \n",
      "2        무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다.  \n",
      "3                           다시 봤을때는 무덤덤했다.  \n",
      "4     8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다.  \n",
      "...                                    ...  \n",
      "1661          이미지 데이터를 가공하는 것이 가장 난이도가 높다.  \n",
      "1662             결말을 보니 분명히 2편이 나올것이 틀림없다.  \n",
      "1663  사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다.  \n",
      "1664            이곳에서 머무르며 로마의 명소들을 방문했습니다.  \n",
      "1665                 난 당신이 떤나 사실도 모르고 있었는데  \n",
      "\n",
      "[1666 rows x 2 columns]\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 798504.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 19.46it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "                                                premise  \\\n",
      "0                             다만 조금 좁아서 케리어를 펼치기 불편합니다.   \n",
      "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요   \n",
      "2                     구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.   \n",
      "3                              몇 번을 다시봐도 볼 때마다 가슴이 저민다.   \n",
      "4          8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.   \n",
      "...                                                 ...   \n",
      "1661  또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...   \n",
      "1662                   결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.   \n",
      "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...   \n",
      "1664                            로마에서 3박4일간 이곳에서 머물렀습니다.   \n",
      "1665                             난 당신이 떠날때 길 하나도 못 건넜는데   \n",
      "\n",
      "                                hypothesis  \n",
      "0                    케리어를 펼치기에 공간이 충분했습니다.  \n",
      "1               시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요  \n",
      "2        무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다.  \n",
      "3                           다시 봤을때는 무덤덤했다.  \n",
      "4     8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다.  \n",
      "...                                    ...  \n",
      "1661          이미지 데이터를 가공하는 것이 가장 난이도가 높다.  \n",
      "1662             결말을 보니 분명히 2편이 나올것이 틀림없다.  \n",
      "1663  사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다.  \n",
      "1664            이곳에서 머무르며 로마의 명소들을 방문했습니다.  \n",
      "1665                 난 당신이 떤나 사실도 모르고 있었는데  \n",
      "\n",
      "[1666 rows x 2 columns]\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 836450.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 19.73it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "                                                premise  \\\n",
      "0                             다만 조금 좁아서 케리어를 펼치기 불편합니다.   \n",
      "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요   \n",
      "2                     구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.   \n",
      "3                              몇 번을 다시봐도 볼 때마다 가슴이 저민다.   \n",
      "4          8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.   \n",
      "...                                                 ...   \n",
      "1661  또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...   \n",
      "1662                   결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.   \n",
      "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...   \n",
      "1664                            로마에서 3박4일간 이곳에서 머물렀습니다.   \n",
      "1665                             난 당신이 떠날때 길 하나도 못 건넜는데   \n",
      "\n",
      "                                hypothesis  \n",
      "0                    케리어를 펼치기에 공간이 충분했습니다.  \n",
      "1               시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요  \n",
      "2        무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다.  \n",
      "3                           다시 봤을때는 무덤덤했다.  \n",
      "4     8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다.  \n",
      "...                                    ...  \n",
      "1661          이미지 데이터를 가공하는 것이 가장 난이도가 높다.  \n",
      "1662             결말을 보니 분명히 2편이 나올것이 틀림없다.  \n",
      "1663  사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다.  \n",
      "1664            이곳에서 머무르며 로마의 명소들을 방문했습니다.  \n",
      "1665                 난 당신이 떤나 사실도 모르고 있었는데  \n",
      "\n",
      "[1666 rows x 2 columns]\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 710277.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 18.11it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "                                                premise  \\\n",
      "0                             다만 조금 좁아서 케리어를 펼치기 불편합니다.   \n",
      "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요   \n",
      "2                     구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.   \n",
      "3                              몇 번을 다시봐도 볼 때마다 가슴이 저민다.   \n",
      "4          8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.   \n",
      "...                                                 ...   \n",
      "1661  또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...   \n",
      "1662                   결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.   \n",
      "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...   \n",
      "1664                            로마에서 3박4일간 이곳에서 머물렀습니다.   \n",
      "1665                             난 당신이 떠날때 길 하나도 못 건넜는데   \n",
      "\n",
      "                                hypothesis  \n",
      "0                    케리어를 펼치기에 공간이 충분했습니다.  \n",
      "1               시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요  \n",
      "2        무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다.  \n",
      "3                           다시 봤을때는 무덤덤했다.  \n",
      "4     8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다.  \n",
      "...                                    ...  \n",
      "1661          이미지 데이터를 가공하는 것이 가장 난이도가 높다.  \n",
      "1662             결말을 보니 분명히 2편이 나올것이 틀림없다.  \n",
      "1663  사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다.  \n",
      "1664            이곳에서 머무르며 로마의 명소들을 방문했습니다.  \n",
      "1665                 난 당신이 떤나 사실도 모르고 있었는데  \n",
      "\n",
      "[1666 rows x 2 columns]\n",
      "test load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [00:00, 874119.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 18.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_logit = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1        neutral\n",
      "2         2     entailment\n",
      "3         3  contradiction\n",
      "4         4  contradiction\n",
      "...     ...            ...\n",
      "1661   1661        neutral\n",
      "1662   1662     entailment\n",
      "1663   1663        neutral\n",
      "1664   1664        neutral\n",
      "1665   1665        neutral\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "out = [list(label_dict.keys())[_] for _ in final_logit.argmax(1)]\n",
    "\n",
    "sub['label'] = out\n",
    "print(sub)\n",
    "# preds\n",
    "sub.to_csv(f'./submission/final_submission_roberta-large_200_explain.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
